{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "d3d76a0f-d518-46ef-84c9-929961e566d0",
    "_uuid": "09bb88f6c67db0768db33b59edd0027f23cd69bb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['test.csv', 'test_sample.csv', 'test_supplement.csv', 'train.csv', 'train_sample.csv', 'y.pickle']\n",
      "Loading Data\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "import os\n",
    "print(os.listdir(\"../input\"))\n",
    "\n",
    "# Any results you write to the current directory are saved as output.\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import lightgbm as lgb\n",
    "import gc\n",
    "\n",
    "\n",
    "path = '../input/' \n",
    "path_train = path + 'train.csv'\n",
    "path_test = path + 'test.csv'\n",
    "\n",
    "train_cols = ['ip', 'app', 'device', 'os', 'channel', 'click_time', 'is_attributed']\n",
    "test_cols = ['ip', 'app', 'device', 'os', 'channel', 'click_time']\n",
    "\n",
    "dtypes = {\n",
    "        'ip'            : 'uint32',\n",
    "        'app'           : 'uint16',\n",
    "        'device'        : 'uint16',\n",
    "        'os'            : 'uint16',\n",
    "        'channel'       : 'uint16',\n",
    "        'is_attributed' : 'uint8',\n",
    "        'click_id'      : 'uint32'\n",
    "        }\n",
    "        \n",
    "skip = range(1, 140000000)\n",
    "print(\"Loading Data\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def prepare_data(data, training_day, profile_days, sample_count=1, only_for_ip_with_hist = False):\n",
    "    if sample_count != 1:\n",
    "        #sample 1/4 of the data:\n",
    "        data = data.set_index('ip').loc[lambda x: (x.index + 401) % sample_count == 0].reset_index()\n",
    "        len_train = len(data)\n",
    "        print('len after sample:', len_train)\n",
    "\n",
    "    train_ip_contains_training_day = data.groupby('ip').filter(lambda x: x['day'].max() == training_day)\n",
    "\n",
    "    print('train_ip_contains_training_day', train_ip_contains_training_day)\n",
    "    print('train_ip_contains_training_day unique ips:', len(train_ip_contains_training_day['ip'].unique()))\n",
    "\n",
    "    if only_for_ip_with_hist:\n",
    "        train = train_ip_contains_training_day.groupby('ip').filter(lambda x: x['day'].min() < training_day)\n",
    "        #train = train_ip_contains_training_day.query('day == {}'.format(training_day))\n",
    "\n",
    "    train_ip_contains_training_day = train_ip_contains_training_day \\\n",
    "        .query('day < {0} & day > {1}'.format(training_day, training_day - 1 - profile_days) )\n",
    "    print('train_ip_contains_training_day unique ips:', len(train_ip_contains_training_day['ip'].unique()))\n",
    "\n",
    "    print('split attributed data:')\n",
    "    train_ip_contains_training_day_attributed = train_ip_contains_training_day.query('is_attributed == 1')\n",
    "    print('len:',len(train_ip_contains_training_day_attributed))\n",
    "\n",
    "    #only use data on 9 to train, but data before 9 as features\n",
    "    if not only_for_ip_with_hist:\n",
    "        train = data.query('day == {}'.format(training_day))\n",
    "    print('training data len:', len(train))\n",
    "    \n",
    "    return train, train_ip_contains_training_day, train_ip_contains_training_day_attributed\n",
    "\n",
    "def add_statistic_feature(group_by_cols, training, training_hist, training_hist_attribution, \n",
    "                          with_hist, counting_col='channel', cast_type=False, qcut_count=0):\n",
    "    features_added = []\n",
    "    feature_name_added = '_'.join(group_by_cols) + 'count'\n",
    "    print('count ip with group by:', group_by_cols)\n",
    "    n_chans = training[group_by_cols + [counting_col]].groupby(by=group_by_cols)[[counting_col]] \\\n",
    "        .count().reset_index().rename(columns={counting_col: feature_name_added})\n",
    "    training = training.merge(n_chans, on=group_by_cols, how='left')\n",
    "    del n_chans\n",
    "    gc.collect()\n",
    "    training[feature_name_added] = training[feature_name_added].astype('uint16')\n",
    "    if qcut_count != 0:\n",
    "        print('before qcut', feature_name_added, training[feature_name_added].describe())\n",
    "        quantile_cut = training[feature_name_added].quantile(qcut_count)\n",
    "        training[feature_name_added] = training[feature_name_added].apply(lambda x: x if x < quantile_cut else 65535).astype('uint16')\n",
    "        print('after qcut', feature_name_added, training[feature_name_added].describe())\n",
    "\n",
    "    features_added.append(feature_name_added)\n",
    "    \n",
    "    if with_hist:\n",
    "        print('count ip with group by in hist data:', group_by_cols)\n",
    "        feature_name_added = '_'.join(group_by_cols) + \"count_in_hist\"\n",
    "        n_chans = training_hist[group_by_cols + [counting_col]].groupby(by=group_by_cols)[[counting_col]] \\\n",
    "            .count().reset_index().rename(columns={counting_col: feature_name_added})\n",
    "        training = training.merge(n_chans, on=group_by_cols, how='left')\n",
    "        del n_chans\n",
    "        gc.collect()\n",
    "        \n",
    "        if qcut_count != 0:\n",
    "            print('before qcut', feature_name_added, training[feature_name_added].describe())\n",
    "            quantile_cut = training[feature_name_added].quantile(qcut_count)\n",
    "            training[feature_name_added] = training[feature_name_added].apply(lambda x: x if x < quantile_cut else -1)\n",
    "            print('after qcut', feature_name_added, training[feature_name_added].describe())\n",
    "\n",
    "        if cast_type:\n",
    "            training[feature_name_added] = training[feature_name_added].fillna(-1).astype('uint16')\n",
    "        print('count ip attribution with group by in hist data:', group_by_cols)\n",
    "        feature_name_added1 = '_'.join(group_by_cols) + \"count_attribution_in_hist\"\n",
    "        n_chans = training_hist_attribution[group_by_cols + [counting_col]] \\\n",
    "            .groupby(by=group_by_cols)[[counting_col]] \\\n",
    "            .count().reset_index().rename(columns={counting_col: feature_name_added1 })\n",
    "        training = training.merge(n_chans, on=group_by_cols, how='left')\n",
    "        del n_chans\n",
    "        gc.collect()\n",
    "\n",
    "        \n",
    "        if qcut_count != 0:\n",
    "            print('before qcut', feature_name_added1, training[feature_name_added1].describe())\n",
    "            quantile_cut = training[feature_name_added1].quantile(qcut_count)\n",
    "            training[feature_name_added1] = training[feature_name_added1].apply(lambda x: x if x < quantile_cut else -1)\n",
    "            print('after qcut', feature_name_added1, training[feature_name_added1].describe())\n",
    "            \n",
    "        if cast_type:\n",
    "            training[feature_name_added1] = training[feature_name_added1].fillna(-1).astype('uint16')\n",
    "        #training[feature_name_added1] = training[feature_name_added1].astype('uint16')\n",
    "                            \n",
    "        feature_name_added2 = '_'.join(group_by_cols) + \"count_attribution_rate_in_hist\"\n",
    "        training[feature_name_added2] = \\\n",
    "            training[feature_name_added1] / training[feature_name_added] * 10000.0\n",
    "            \n",
    "        if cast_type:\n",
    "            training[feature_name_added2] = training[feature_name_added2].fillna(-1).astype('uint16')\n",
    "            \n",
    "        features_added.append(feature_name_added)\n",
    "        features_added.append(feature_name_added1)\n",
    "        features_added.append('_'.join(group_by_cols) + \"count_attribution_rate_in_hist\")\n",
    "        \n",
    "    print('added features:', features_added)\n",
    "                                               \n",
    "    return training, features_added\n",
    "def add_statistic_feature1(group_by_cols, training, training_hist, training_hist_attribution, \n",
    "                          with_hist, counting_col='channel'):\n",
    "    features_added = []\n",
    "    feature_name_added = '_'.join(group_by_cols) + 'count'\n",
    "    print('count ip with group by:', group_by_cols)\n",
    "    n_chans = training[group_by_cols + [counting_col]].groupby(by=group_by_cols)[[counting_col]] \\\n",
    "        .count().reset_index().rename(columns={counting_col: feature_name_added})\n",
    "    training = training.merge(n_chans, on=group_by_cols, how='left')\n",
    "    del n_chans\n",
    "    gc.collect()\n",
    "    training[feature_name_added] = training[feature_name_added].astype('uint16')\n",
    "    features_added.append(feature_name_added)\n",
    "    \n",
    "    if with_hist:\n",
    "        print('count ip with group by in hist data:', group_by_cols)\n",
    "        feature_name_added = '_'.join(group_by_cols) + \"count_in_hist\"\n",
    "        n_chans = training_hist[group_by_cols + [counting_col]].groupby(by=group_by_cols)[[counting_col]] \\\n",
    "            .count().reset_index().rename(columns={counting_col: feature_name_added})\n",
    "        training = training.merge(n_chans, on=group_by_cols, how='left')\n",
    "        del n_chans\n",
    "        gc.collect()\n",
    "        #training[feature_name_added] = training[feature_name_added].astype('uint16')\n",
    "        print('count ip attribution with group by in hist data:', group_by_cols)\n",
    "        feature_name_added1 = '_'.join(group_by_cols) + \"count_attribution_in_hist\"\n",
    "        n_chans = training_hist_attribution[group_by_cols + [counting_col]] \\\n",
    "            .groupby(by=group_by_cols)[[counting_col]] \\\n",
    "            .count().reset_index().rename(columns={counting_col: feature_name_added1 })\n",
    "        training = training.merge(n_chans, on=group_by_cols, how='left')\n",
    "        del n_chans\n",
    "        gc.collect()\n",
    "        #training[feature_name_added1] = training[feature_name_added1].astype('uint16')\n",
    "                                               \n",
    "        training['_'.join(group_by_cols) + \"count_attribution_rate_in_hist\"] = \\\n",
    "            training[feature_name_added1] / training[feature_name_added]\n",
    "            \n",
    "        features_added.append(feature_name_added)\n",
    "        features_added.append(feature_name_added1)\n",
    "        features_added.append('_'.join(group_by_cols) + \"count_attribution_rate_in_hist\")\n",
    "        \n",
    "    print('added features:', features_added)\n",
    "                                               \n",
    "    return training, features_added\n",
    "\n",
    "def generate_counting_history_features(data, history, history_attribution, remove_hist_profile_count=0):\n",
    "        \n",
    "    new_features = []\n",
    "    \n",
    "    # Count by IP,DAY,HOUR\n",
    "    print('a given IP address within each hour...')\n",
    "    data, features_added = add_statistic_feature(['ip','day','hour'], data, history, history_attribution, False)\n",
    "    new_features = new_features + features_added\n",
    "    gc.collect()\n",
    "\n",
    "    # Count by IP and APP\n",
    "    data, features_added = add_statistic_feature(['ip','app'], data, history, history_attribution, True)\n",
    "    new_features = new_features + features_added\n",
    "    \n",
    "    # Count by IP and channel\n",
    "    data, features_added = add_statistic_feature(['ip','channel'], data, history, history_attribution, True, counting_col='os')\n",
    "    new_features = new_features + features_added\n",
    "    \n",
    "    # Count by IP and channel app\n",
    "    data, features_added = add_statistic_feature(['ip','channel', 'app'], data, history, history_attribution, True, counting_col='os')\n",
    "    new_features = new_features + features_added\n",
    "    \n",
    "    data, features_added  = add_statistic_feature(['ip','app','os'], data, history, history_attribution, True)\n",
    "    new_features = new_features + features_added\n",
    "\n",
    "    #######\n",
    "    # Count by IP\n",
    "    data, features_added  = add_statistic_feature(['ip'], data, history, history_attribution, True)\n",
    "    new_features = new_features + features_added\n",
    "\n",
    "    # Count by IP HOUR CHANNEL                                               \n",
    "    data, features_added  = add_statistic_feature(['ip','hour','channel'], \\\n",
    "        data, history, history_attribution, True, counting_col='os')\n",
    "    new_features = new_features + features_added\n",
    "\n",
    "    # Count by IP HOUR Device\n",
    "    data, features_added  = add_statistic_feature(['ip','hour','os'], \\\n",
    "        data, history, history_attribution, True)\n",
    "    new_features = new_features + features_added\n",
    "\n",
    "    data, features_added  = add_statistic_feature(['ip','hour','app'], \\\n",
    "        data, history, history_attribution, True)\n",
    "    new_features = new_features + features_added\n",
    "\n",
    "    data, features_added  = add_statistic_feature(['ip','hour','device'], \\\n",
    "        data, history, history_attribution, True)\n",
    "    new_features = new_features + features_added\n",
    "    \n",
    "    if remove_hist_profile_count != 0:\n",
    "        data = data.query('ip_count_in_hist > {}'.format(remove_hist_profile_count))\n",
    "    \n",
    "    return data, new_features\n",
    "\n",
    "\n",
    "for_test1 = False\n",
    "\n",
    "if for_test1:\n",
    "\n",
    "    lgb_model = lgb.Booster(model_file='model1.txt')\n",
    "    #prepare test data:\n",
    "    train = pd.read_csv(path_train, dtype=dtypes,\n",
    "            header=0,usecols=train_cols,parse_dates=[\"click_time\"])#.sample(1000)\n",
    "    test = pd.read_csv(path_test, dtype=dtypes, header=0,\n",
    "            usecols=test_cols,parse_dates=[\"click_time\"])#.sample(1000)\n",
    "    train=train.append(test)\n",
    "    del test\n",
    "    gc.collect()\n",
    "    print(\"Creating new time features in train: 'hour' and 'day'...\")\n",
    "    train['hour'] = train[\"click_time\"].dt.hour.astype('uint8')\n",
    "    train['day'] = train[\"click_time\"].dt.day.astype('uint8')\n",
    "    \n",
    "    train, train_ip_contains_training_day, train_ip_contains_training_day_attributed = \\\n",
    "        prepare_data(train, 10, 3, 1, only_for_ip_with_hist=True)\n",
    "\n",
    "    train, new_features = generate_counting_history_features(train, train_ip_contains_training_day, \n",
    "                                                             train_ip_contains_training_day_attributed, 4)\n",
    "\n",
    "    print('test data:', train)\n",
    "    print('new features:', new_features)\n",
    "    print(\"Preparing data for submission...\")\n",
    "\n",
    "    submit = pd.read_csv(path_test, dtype='int', usecols=['click_id'])\n",
    "    print('submit test len:', len(submit))\n",
    "  \n",
    "\n",
    "    categorical = ['app', 'device', 'os', 'channel', 'hour']\n",
    "\n",
    "    predictors1 = categorical + new_features\n",
    "    print(\"Predicting the submission data...\")\n",
    "    submit['is_attributed'] = lgb_model.predict(train[predictors1], num_iteration=lgb_model.best_iteration)\n",
    "\n",
    "    print(\"Writing the submission data into a csv file...\")\n",
    "\n",
    "    submit.to_csv(\"submission.csv\",index=False)\n",
    "\n",
    "    print(\"All done...\")\n",
    "    \n",
    "    import sys\n",
    "    sys.exit(0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "2e57bfc0-e2e4-43a6-8e1c-20fa9a7d3c8d",
    "_uuid": "a74b9eaa05283c2e2d048455fe298507656ca6e7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The initial size of the train set is 184903890\n",
      "The initial size of the test set is 18790469\n",
      "Binding the training and test set together...\n",
      "Creating new time features in train: 'hour' and 'day'...\n",
      "Creating new time features in test: 'hour' and 'day'...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#skiprows=skip, \n",
    "train = pd.read_csv(path_train, dtype=dtypes,\n",
    "        header=0,usecols=train_cols,parse_dates=[\"click_time\"])#.sample(1000)\n",
    "test = pd.read_csv(path_test, dtype=dtypes, header=0,\n",
    "        usecols=test_cols,parse_dates=[\"click_time\"])#.sample(1000)\n",
    "#test['is_attributed'] = -1\n",
    "\n",
    "len_train = len(train)\n",
    "print('The initial size of the train set is', len_train)\n",
    "print('The initial size of the test set is', len(test))\n",
    "print('Binding the training and test set together...')\n",
    "\n",
    "\n",
    "\n",
    "print(\"Creating new time features in train: 'hour' and 'day'...\")\n",
    "train['hour'] = train[\"click_time\"].dt.hour.astype('uint8')\n",
    "train['day'] = train[\"click_time\"].dt.day.astype('uint8')\n",
    "\n",
    "\n",
    "print(\"Creating new time features in test: 'hour' and 'day'...\")\n",
    "test['hour'] = test[\"click_time\"].dt.hour.astype('uint8')\n",
    "test['day'] = test[\"click_time\"].dt.day.astype('uint8')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "3a46577f-c98e-4c7c-9451-cf6809ed74e7",
    "_uuid": "fa0491bc2ffe926edc61206a5d6eaaf06cde247f",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len after sample: 46849705\n",
      "train_ip_contains_training_day               ip  app  device  os  channel          click_time  is_attributed  \\\n",
      "1          18787    3       1  16      379 2017-11-06 14:36:26              0   \n",
      "2         124979    3       1  18      379 2017-11-06 14:40:16              0   \n",
      "3          80447    3       1  19      379 2017-11-06 14:40:51              0   \n",
      "4         134575    3       1  13      379 2017-11-06 14:43:10              0   \n",
      "6         191759    3       1  13      379 2017-11-06 14:44:51              0   \n",
      "7         209663    3       1  13      379 2017-11-06 14:48:55              0   \n",
      "8         208347    3       1  19      379 2017-11-06 14:49:38              0   \n",
      "10         28739    3       1  13      379 2017-11-06 14:50:29              0   \n",
      "11        103175   18       1  17      376 2017-11-06 14:53:23              0   \n",
      "12         74715    3       1  19      379 2017-11-06 14:55:25              0   \n",
      "13        128855    3       1  13      379 2017-11-06 14:58:16              0   \n",
      "14         38763    3       1  13      379 2017-11-06 15:08:04              0   \n",
      "16        107899    6       1  13      459 2017-11-06 15:12:43              0   \n",
      "17         33195    3       1  13      379 2017-11-06 15:13:42              0   \n",
      "18        209663    3       1  19      379 2017-11-06 15:22:08              0   \n",
      "19         88075    3       1  19      379 2017-11-06 15:24:03              0   \n",
      "21        109591    3       1  19      379 2017-11-06 15:30:16              0   \n",
      "22        109591    3       1  19      379 2017-11-06 15:39:16              0   \n",
      "23         63839    3       1  19      379 2017-11-06 15:40:37              0   \n",
      "24          5323   64       1  13      459 2017-11-06 15:40:56              0   \n",
      "25        135659    3       1  13      379 2017-11-06 15:40:59              0   \n",
      "26        120471    3       1  19      379 2017-11-06 15:41:29              0   \n",
      "28         54039   64       1  13      459 2017-11-06 15:41:49              0   \n",
      "30         71367   14       1  13      478 2017-11-06 15:42:08              0   \n",
      "33        125343    3       1  19      379 2017-11-06 15:43:01              0   \n",
      "36        139355    3       1  13      379 2017-11-06 15:43:51              0   \n",
      "41         96699    3       1  25      379 2017-11-06 15:45:45              0   \n",
      "42        109591    3       1  19      379 2017-11-06 15:46:02              0   \n",
      "43         82211    3       1  18      379 2017-11-06 15:46:07              0   \n",
      "44         76683    3       1  13      379 2017-11-06 15:46:22              0   \n",
      "...          ...  ...     ...  ..      ...                 ...            ...   \n",
      "46849675   72951    1       1  77      153 2017-11-09 16:00:00              0   \n",
      "46849676   74839    3       1  19      489 2017-11-09 16:00:00              0   \n",
      "46849677   22491   17       1  18      128 2017-11-09 16:00:00              0   \n",
      "46849678   49383    8       2  20      259 2017-11-09 16:00:00              0   \n",
      "46849679  165259   14       1  13      439 2017-11-09 16:00:00              0   \n",
      "46849680   38783   18       1  36      107 2017-11-09 16:00:00              0   \n",
      "46849681   10959    3       1  19      211 2017-11-09 16:00:00              0   \n",
      "46849682   43855   13       1  19      477 2017-11-09 16:00:00              0   \n",
      "46849683   27159    1       1  22      150 2017-11-09 16:00:00              0   \n",
      "46849684   96903   11       1  13      487 2017-11-09 16:00:00              0   \n",
      "46849685   44299    3       1  25      409 2017-11-09 16:00:00              0   \n",
      "46849686   26583    9       1  19      107 2017-11-09 16:00:00              0   \n",
      "46849687  249327    3       2  22       30 2017-11-09 16:00:00              0   \n",
      "46849688   73487   12       1  22      265 2017-11-09 16:00:00              0   \n",
      "46849689   42103    3       1  14      173 2017-11-09 16:00:00              0   \n",
      "46849690  105475   11       1   1      325 2017-11-09 16:00:00              0   \n",
      "46849691   34439    3       1  16      424 2017-11-09 16:00:00              0   \n",
      "46849692  107155    7       1  13      101 2017-11-09 16:00:00              0   \n",
      "46849693  110347    2       1  10      477 2017-11-09 16:00:00              0   \n",
      "46849694   44067   12       1  19      424 2017-11-09 16:00:00              0   \n",
      "46849695   73487   11       1  13      481 2017-11-09 16:00:00              0   \n",
      "46849696  164415    2       1  19      435 2017-11-09 16:00:00              0   \n",
      "46849697   15431   12       1  17      105 2017-11-09 16:00:00              0   \n",
      "46849698    1003   14       1  19      134 2017-11-09 16:00:00              0   \n",
      "46849699  103743   56       1  55      406 2017-11-09 16:00:00              0   \n",
      "46849700   49383   24       2  20      178 2017-11-09 16:00:00              0   \n",
      "46849701    2463    9       1  17      107 2017-11-09 16:00:00              0   \n",
      "46849702   43763    3       1  25      402 2017-11-09 16:00:00              0   \n",
      "46849703   77979   12       1  23      259 2017-11-09 16:00:00              0   \n",
      "46849704  131635    2       1  17      435 2017-11-09 16:00:00              0   \n",
      "\n",
      "          hour  day  \n",
      "1           14    6  \n",
      "2           14    6  \n",
      "3           14    6  \n",
      "4           14    6  \n",
      "6           14    6  \n",
      "7           14    6  \n",
      "8           14    6  \n",
      "10          14    6  \n",
      "11          14    6  \n",
      "12          14    6  \n",
      "13          14    6  \n",
      "14          15    6  \n",
      "16          15    6  \n",
      "17          15    6  \n",
      "18          15    6  \n",
      "19          15    6  \n",
      "21          15    6  \n",
      "22          15    6  \n",
      "23          15    6  \n",
      "24          15    6  \n",
      "25          15    6  \n",
      "26          15    6  \n",
      "28          15    6  \n",
      "30          15    6  \n",
      "33          15    6  \n",
      "36          15    6  \n",
      "41          15    6  \n",
      "42          15    6  \n",
      "43          15    6  \n",
      "44          15    6  \n",
      "...        ...  ...  \n",
      "46849675    16    9  \n",
      "46849676    16    9  \n",
      "46849677    16    9  \n",
      "46849678    16    9  \n",
      "46849679    16    9  \n",
      "46849680    16    9  \n",
      "46849681    16    9  \n",
      "46849682    16    9  \n",
      "46849683    16    9  \n",
      "46849684    16    9  \n",
      "46849685    16    9  \n",
      "46849686    16    9  \n",
      "46849687    16    9  \n",
      "46849688    16    9  \n",
      "46849689    16    9  \n",
      "46849690    16    9  \n",
      "46849691    16    9  \n",
      "46849692    16    9  \n",
      "46849693    16    9  \n",
      "46849694    16    9  \n",
      "46849695    16    9  \n",
      "46849696    16    9  \n",
      "46849697    16    9  \n",
      "46849698    16    9  \n",
      "46849699    16    9  \n",
      "46849700    16    9  \n",
      "46849701    16    9  \n",
      "46849702    16    9  \n",
      "46849703    16    9  \n",
      "46849704    16    9  \n",
      "\n",
      "[40484575 rows x 9 columns]\n",
      "train_ip_contains_training_day unique ips: 27752\n",
      "train_ip_contains_training_day unique ips: 15322\n",
      "split attributed data:\n",
      "len: 36301\n",
      "training data len: 40212273\n",
      "a given IP address within each hour...\n",
      "count ip with group by: ['ip', 'day', 'hour']\n",
      "added features: ['ip_day_hourcount']\n",
      "count ip with group by: ['ip', 'app']\n",
      "count ip with group by in hist data: ['ip', 'app']\n",
      "count ip attribution with group by in hist data: ['ip', 'app']\n",
      "added features: ['ip_appcount', 'ip_appcount_in_hist', 'ip_appcount_attribution_in_hist', 'ip_appcount_attribution_rate_in_hist']\n",
      "count ip with group by: ['ip', 'channel']\n",
      "count ip with group by in hist data: ['ip', 'channel']\n",
      "count ip attribution with group by in hist data: ['ip', 'channel']\n",
      "added features: ['ip_channelcount', 'ip_channelcount_in_hist', 'ip_channelcount_attribution_in_hist', 'ip_channelcount_attribution_rate_in_hist']\n",
      "count ip with group by: ['ip', 'channel', 'app']\n",
      "count ip with group by in hist data: ['ip', 'channel', 'app']\n",
      "count ip attribution with group by in hist data: ['ip', 'channel', 'app']\n",
      "added features: ['ip_channel_appcount', 'ip_channel_appcount_in_hist', 'ip_channel_appcount_attribution_in_hist', 'ip_channel_appcount_attribution_rate_in_hist']\n",
      "count ip with group by: ['ip', 'app', 'os']\n",
      "count ip with group by in hist data: ['ip', 'app', 'os']\n",
      "count ip attribution with group by in hist data: ['ip', 'app', 'os']\n",
      "added features: ['ip_app_oscount', 'ip_app_oscount_in_hist', 'ip_app_oscount_attribution_in_hist', 'ip_app_oscount_attribution_rate_in_hist']\n",
      "count ip with group by: ['ip']\n",
      "count ip with group by in hist data: ['ip']\n",
      "count ip attribution with group by in hist data: ['ip']\n",
      "added features: ['ipcount', 'ipcount_in_hist', 'ipcount_attribution_in_hist', 'ipcount_attribution_rate_in_hist']\n",
      "count ip with group by: ['ip', 'hour', 'channel']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count ip with group by in hist data: ['ip', 'hour', 'channel']\n",
      "count ip attribution with group by in hist data: ['ip', 'hour', 'channel']\n",
      "added features: ['ip_hour_channelcount', 'ip_hour_channelcount_in_hist', 'ip_hour_channelcount_attribution_in_hist', 'ip_hour_channelcount_attribution_rate_in_hist']\n",
      "count ip with group by: ['ip', 'hour', 'os']\n",
      "count ip with group by in hist data: ['ip', 'hour', 'os']\n",
      "count ip attribution with group by in hist data: ['ip', 'hour', 'os']\n",
      "added features: ['ip_hour_oscount', 'ip_hour_oscount_in_hist', 'ip_hour_oscount_attribution_in_hist', 'ip_hour_oscount_attribution_rate_in_hist']\n",
      "count ip with group by: ['ip', 'hour', 'app']\n",
      "count ip with group by in hist data: ['ip', 'hour', 'app']\n",
      "count ip attribution with group by in hist data: ['ip', 'hour', 'app']\n",
      "added features: ['ip_hour_appcount', 'ip_hour_appcount_in_hist', 'ip_hour_appcount_attribution_in_hist', 'ip_hour_appcount_attribution_rate_in_hist']\n",
      "count ip with group by: ['ip', 'hour', 'device']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#test['hour'] = test[\"click_time\"].dt.hour.astype('uint8')\n",
    "#test['day'] = test[\"click_time\"].dt.day.astype('uint8')\n",
    "\n",
    "train, train_ip_contains_training_day, train_ip_contains_training_day_attributed = \\\n",
    "    prepare_data(train, 9, 3, 4, only_for_ip_with_hist=True)\n",
    "\n",
    "train, new_features = generate_counting_history_features(train, train_ip_contains_training_day, \n",
    "                                                         train_ip_contains_training_day_attributed, 4)\n",
    "\n",
    "print('train data:', train)\n",
    "print('new features:', new_features)\n",
    "\n",
    "val = train.set_index('ip').loc[lambda x: (x.index) % 17 == 0].reset_index()\n",
    "print(val)\n",
    "print('The size of the validation set is ', len(val))\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "train = train.set_index('ip').loc[lambda x: (x.index) % 17 != 0].reset_index()\n",
    "print('The size of the train set is ', len(train))\n",
    "\n",
    "target = 'is_attributed'\n",
    "train[target] = train[target].astype('uint8')\n",
    "train.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "predictors0 = ['device', 'app', 'os', 'channel', 'hour', # Starter Vars, Then new features below\n",
    "              'ip_day_hourcount','ipcount','ip_appcount', 'ip_app_oscount',\n",
    "              \"ip_hour_channelcount\", \"ip_hour_oscount\", \"ip_hour_appcount\",\"ip_hour_devicecount\"]\n",
    "\n",
    "categorical = ['app', 'device', 'os', 'channel', 'hour']\n",
    "\n",
    "predictors1 = categorical + new_features\n",
    "#for ii in new_features:\n",
    "#    predictors1 = predictors1 + ii\n",
    "#print(predictors1)\n",
    "gc.collect()\n",
    "\n",
    "#train.fillna(value={x:-1 for x in new_features})\n",
    "\n",
    "print(\"Preparing the datasets for training...\")\n",
    "\n",
    "params = {\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'binary',\n",
    "    'metric': 'auc',\n",
    "    'learning_rate': 0.1,\n",
    "    'num_leaves': 7,  \n",
    "    'max_depth': 4,  \n",
    "    'min_child_samples': 100,  \n",
    "    'max_bin': 150,  \n",
    "    'subsample': 0.7,  \n",
    "    'subsample_freq': 1,  \n",
    "    'colsample_bytree': 0.7,  \n",
    "    'min_child_weight': 0,  \n",
    "    'subsample_for_bin': 200000,  \n",
    "    'min_split_gain': 0,  \n",
    "    'reg_alpha': 0,  \n",
    "    'reg_lambda': 0,  \n",
    "    'nthread': 5,\n",
    "    'verbose': 9,\n",
    "    #'is_unbalance': True,\n",
    "    'scale_pos_weight':99 \n",
    "    }\n",
    "    \n",
    "predictors_to_train = [predictors1]\n",
    "\n",
    "for predictors in predictors_to_train:\n",
    "    print('training with :', predictors)\n",
    "    #print('training data: ', train[predictors].values)\n",
    "    #print('validation data: ', val[predictors].values)\n",
    "    dtrain = lgb.Dataset(train[predictors].values, label=train[target].values,\n",
    "                          feature_name=predictors,\n",
    "                          categorical_feature=categorical\n",
    "                          )\n",
    "    dvalid = lgb.Dataset(val[predictors].values, label=val[target].values,\n",
    "                          feature_name=predictors,\n",
    "                          categorical_feature=categorical\n",
    "                          )\n",
    "\n",
    "    evals_results = {}\n",
    "    print(\"Training the model...\")\n",
    "\n",
    "    lgb_model = lgb.train(params, \n",
    "                     dtrain, \n",
    "                     valid_sets=[dtrain, dvalid], \n",
    "                     valid_names=['train','valid'], \n",
    "                     evals_result=evals_results, \n",
    "                     num_boost_round=1000,\n",
    "                     early_stopping_rounds=30,\n",
    "                     verbose_eval=50, \n",
    "                     feval=None)\n",
    "\n",
    "    #del train\n",
    "    #del val\n",
    "    #gc.collect()\n",
    "\n",
    "    # Nick's Feature Importance Plot\n",
    "    import matplotlib.pyplot as plt\n",
    "    f, ax = plt.subplots(figsize=[7,10])\n",
    "    lgb.plot_importance(lgb_model, ax=ax, max_num_features=len(predictors))\n",
    "    plt.title(\"Light GBM Feature Importance\")\n",
    "    plt.savefig('feature_import.png')\n",
    "\n",
    "    # Feature names:\n",
    "    print('Feature names:', lgb_model.feature_name())\n",
    "    # Feature importances:\n",
    "    print('Feature importances:', list(lgb_model.feature_importance()))\n",
    "\n",
    "    feature_imp = pd.DataFrame(lgb_model.feature_name(),list(lgb_model.feature_importance()))\n",
    "    \n",
    "    print('saving model...')\n",
    "    lgb_model.save_model('model.txt')\n",
    "    \n",
    "    print('model saved')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "952d1edf-174f-4048-92ac-7d5115c2f1a6",
    "_uuid": "73214a3ae8e59ca6b1c122e6d31c1284364c8eb4",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#persist training data\n",
    "persist_data = False\n",
    "if persist_data:\n",
    "    train.to_csv(\"train.csv.gzip\",index=False,compression='gzip')\n",
    "    val.to_csv(\"val.csv.gzip\",index=False,compression='gzip')\n",
    "\n",
    "    import pickle\n",
    "    pickle.dump(predictors1, open('predictors1.pickle','wb'))\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for_test = False\n",
    "\n",
    "if for_test:\n",
    "    #del train\n",
    "    #del test\n",
    "    del train_ip_contains_training_day\n",
    "    del train_ip_contains_training_day_attributed\n",
    "    del val\n",
    "    gc.collect()\n",
    "\n",
    "    #prepare test data:\n",
    "    train = pd.read_csv(path_train, dtype=dtypes,\n",
    "            header=0,usecols=train_cols,parse_dates=[\"click_time\"])#.sample(1000)\n",
    "    test = pd.read_csv(path_test, dtype=dtypes, header=0,\n",
    "            usecols=test_cols,parse_dates=[\"click_time\"])#.sample(1000)\n",
    "    train=train.append(test)\n",
    "    del test\n",
    "    gc.collect()\n",
    "    print(\"Creating new time features in train: 'hour' and 'day'...\")\n",
    "    train['hour'] = train[\"click_time\"].dt.hour.astype('uint8')\n",
    "    train['day'] = train[\"click_time\"].dt.day.astype('uint8')\n",
    "    \n",
    "    train, train_ip_contains_training_day, train_ip_contains_training_day_attributed = \\\n",
    "        prepare_data(train, 10, 3, 1, only_for_ip_with_hist=True)\n",
    "\n",
    "    train, new_features = generate_counting_history_features(train, train_ip_contains_training_day, \n",
    "                                                             train_ip_contains_training_day_attributed, 4)\n",
    "\n",
    "    print('test data:', train)\n",
    "    print('new features:', new_features)\n",
    "    print(\"Preparing data for submission...\")\n",
    "\n",
    "    submit = pd.read_csv(path_test, dtype='int', usecols=['click_id'])\n",
    "    print('submit test len:', len(submit))\n",
    "    print(\"Predicting the submission data...\")\n",
    "    submit['is_attributed'] = lgb_model.predict(train[predictors1], num_iteration=lgb_model.best_iteration)\n",
    "\n",
    "    print(\"Writing the submission data into a csv file...\")\n",
    "\n",
    "    submit.to_csv(\"submission.csv\",index=False)\n",
    "\n",
    "    print(\"All done...\")\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "76224c53-eeaf-408f-9591-846eb0b22157",
    "_uuid": "88d268bb4880465a08872ffd711cb41891ddd1a2",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "load_persist = False\n",
    "if load_persist:\n",
    "    print('reading train.csv.gzip')\n",
    "\n",
    "    import numpy as np # linear algebra\n",
    "    import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "    train = pd.read_csv('train.csv.gzip',compression='gzip')\n",
    "\n",
    "    print('read done')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "83eb5aeb-aaf6-471b-82ef-b13f6b63909c",
    "_uuid": "84a23892f3e9a731740ff0d143c53ea40b7bc031",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#print(train.groupby('ip_channel_appcount_attribution_rate_in_hist')['ip'].count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "9aed49d4-c504-470c-8af1-15e77db07a62",
    "_uuid": "3872f4faab6ccdac83dd3c0326ab7b9b2d437a3b",
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "Another CTR comp and so i suspect libffm will play its part, after all it is an atomic bomb for this kind of stuff.\n",
    "A sci-kit learn inspired script to convert pandas dataframes into libFFM style data.\n",
    "\n",
    "The script is fairly hacky (hey thats Kaggle) and takes a little while to run a huge dataset.\n",
    "The key to using this class is setting up the features dtypes correctly for output (ammend transform to suit your needs)\n",
    "\n",
    "Example below\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "class FFMFormatPandas:\n",
    "    def __init__(self):\n",
    "        self.field_index_ = None\n",
    "        self.feature_index_ = None\n",
    "        self.y = None\n",
    "\n",
    "    def fit(self, df, y=None):\n",
    "        self.y = y\n",
    "        df_ffm = df[df.columns.difference([self.y])]\n",
    "        if self.field_index_ is None:\n",
    "            self.field_index_ = {col: i for i, col in enumerate(df_ffm)}\n",
    "\n",
    "        if self.feature_index_ is not None:\n",
    "            last_idx = max(list(self.feature_index_.values()))\n",
    "\n",
    "        if self.feature_index_ is None:\n",
    "            self.feature_index_ = dict()\n",
    "            last_idx = 0\n",
    "\n",
    "        for col in df.columns:\n",
    "            vals = df[col].unique()\n",
    "            for val in vals:\n",
    "                if pd.isnull(val):\n",
    "                    continue\n",
    "                name = '{}_{}'.format(col, val)\n",
    "                if name not in self.feature_index_:\n",
    "                    self.feature_index_[name] = last_idx\n",
    "                    last_idx += 1\n",
    "            self.feature_index_[col] = last_idx\n",
    "            last_idx += 1\n",
    "        return self\n",
    "\n",
    "    def fit_transform(self, df, y=None):\n",
    "        self.fit(df, y)\n",
    "        return self.transform(df)\n",
    "\n",
    "    def transform_row_(self, row, t):\n",
    "        ffm = []\n",
    "        if self.y != None:\n",
    "            ffm.append(str(row.loc[row.index == self.y][0]))\n",
    "        if self.y is None:\n",
    "            ffm.append(str(0))\n",
    "\n",
    "        for col, val in row.loc[row.index != self.y].to_dict().items():\n",
    "            col_type = t[col]\n",
    "            name = '{}_{}'.format(col, val)\n",
    "            if col_type.kind ==  'O':\n",
    "                ffm.append('{}:{}:1'.format(self.field_index_[col], self.feature_index_[name]))\n",
    "            elif col_type.kind == 'i':\n",
    "                ffm.append('{}:{}:{}'.format(self.field_index_[col], self.feature_index_[col], val))\n",
    "        return ' '.join(ffm)\n",
    "\n",
    "    def transform(self, df):\n",
    "        t = df.dtypes.to_dict()\n",
    "        return pd.Series({idx: self.transform_row_(row, t) for idx, row in df.iterrows()})\n",
    "\n",
    "use_FFM = False\n",
    "\n",
    "if use_FFM:\n",
    "    ffm_train = FFMFormatPandas()\n",
    "    ffm_train_data = ffm_train.fit_transform(train, y='is_attributed')\n",
    "\n",
    "    print('FFM data:',ffm_train_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
