{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "d3d76a0f-d518-46ef-84c9-929961e566d0",
    "_uuid": "09bb88f6c67db0768db33b59edd0027f23cd69bb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['test.csv', 'test_sample.csv', 'test_supplement.csv', 'train.csv', 'train_sample.csv', 'y.pickle']\n",
      "Loading Data\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "import os\n",
    "print(os.listdir(\"../input\"))\n",
    "\n",
    "# Any results you write to the current directory are saved as output.\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import lightgbm as lgb\n",
    "import gc\n",
    "\n",
    "\n",
    "path = '../input/' \n",
    "path_train = path + 'train.csv'\n",
    "path_test = path + 'test.csv'\n",
    "\n",
    "train_cols = ['ip', 'app', 'device', 'os', 'channel', 'click_time', 'is_attributed']\n",
    "test_cols = ['ip', 'app', 'device', 'os', 'channel', 'click_time']\n",
    "\n",
    "dtypes = {\n",
    "        'ip'            : 'uint32',\n",
    "        'app'           : 'uint16',\n",
    "        'device'        : 'uint16',\n",
    "        'os'            : 'uint16',\n",
    "        'channel'       : 'uint16',\n",
    "        'is_attributed' : 'uint8',\n",
    "        'click_id'      : 'uint32'\n",
    "        }\n",
    "        \n",
    "skip = range(1, 140000000)\n",
    "print(\"Loading Data\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def prepare_data(data, training_day, profile_days, sample_count=1, only_for_ip_with_hist = False):\n",
    "    if sample_count != 1:\n",
    "        #sample 1/4 of the data:\n",
    "        data = data.set_index('ip').loc[lambda x: (x.index + 401) % sample_count == 0].reset_index()\n",
    "        len_train = len(data)\n",
    "        print('len after sample:', len_train)\n",
    "\n",
    "    train_ip_contains_training_day = data.groupby('ip').filter(lambda x: x['day'].max() == training_day)\n",
    "\n",
    "    print('train_ip_contains_training_day', train_ip_contains_training_day)\n",
    "    print('train_ip_contains_training_day unique ips:', len(train_ip_contains_training_day['ip'].unique()))\n",
    "\n",
    "    if only_for_ip_with_hist:\n",
    "        train = train_ip_contains_training_day.groupby('ip').filter(lambda x: x['day'].min() < training_day)\n",
    "        #train = train_ip_contains_training_day.query('day == {}'.format(training_day))\n",
    "\n",
    "    train_ip_contains_training_day = train_ip_contains_training_day \\\n",
    "        .query('day < {0} & day > {1}'.format(training_day, training_day - 1 - profile_days) )\n",
    "    print('train_ip_contains_training_day unique ips:', len(train_ip_contains_training_day['ip'].unique()))\n",
    "\n",
    "    print('split attributed data:')\n",
    "    train_ip_contains_training_day_attributed = train_ip_contains_training_day.query('is_attributed == 1')\n",
    "    print('len:',len(train_ip_contains_training_day_attributed))\n",
    "\n",
    "    #only use data on 9 to train, but data before 9 as features\n",
    "    if not only_for_ip_with_hist:\n",
    "        train = data.query('day == {}'.format(training_day))\n",
    "    print('training data len:', len(train))\n",
    "    \n",
    "    return train, train_ip_contains_training_day, train_ip_contains_training_day_attributed\n",
    "\n",
    "def add_statistic_feature(group_by_cols, training, training_hist, training_hist_attribution, \n",
    "                          with_hist, counting_col='channel', cast_type=False, qcut_count=0):\n",
    "    features_added = []\n",
    "    feature_name_added = '_'.join(group_by_cols) + 'count'\n",
    "    print('count ip with group by:', group_by_cols)\n",
    "    n_chans = training[group_by_cols + [counting_col]].groupby(by=group_by_cols)[[counting_col]] \\\n",
    "        .count().reset_index().rename(columns={counting_col: feature_name_added})\n",
    "    training = training.merge(n_chans, on=group_by_cols, how='left')\n",
    "    del n_chans\n",
    "    gc.collect()\n",
    "    training[feature_name_added] = training[feature_name_added].astype('uint16')\n",
    "    if qcut_count != 0:\n",
    "        print('before qcut', feature_name_added, training[feature_name_added].describe())\n",
    "        quantile_cut = training[feature_name_added].quantile(qcut_count)\n",
    "        training[feature_name_added] = training[feature_name_added].apply(lambda x: x if x < quantile_cut else 65535).astype('uint16')\n",
    "        print('after qcut', feature_name_added, training[feature_name_added].describe())\n",
    "\n",
    "    features_added.append(feature_name_added)\n",
    "    \n",
    "    if with_hist:\n",
    "        print('count ip with group by in hist data:', group_by_cols)\n",
    "        feature_name_added = '_'.join(group_by_cols) + \"count_in_hist\"\n",
    "        n_chans = training_hist[group_by_cols + [counting_col]].groupby(by=group_by_cols)[[counting_col]] \\\n",
    "            .count().reset_index().rename(columns={counting_col: feature_name_added})\n",
    "        training = training.merge(n_chans, on=group_by_cols, how='left')\n",
    "        del n_chans\n",
    "        gc.collect()\n",
    "        \n",
    "        if qcut_count != 0:\n",
    "            print('before qcut', feature_name_added, training[feature_name_added].describe())\n",
    "            quantile_cut = training[feature_name_added].quantile(qcut_count)\n",
    "            training[feature_name_added] = training[feature_name_added].apply(lambda x: x if x < quantile_cut else -1)\n",
    "            print('after qcut', feature_name_added, training[feature_name_added].describe())\n",
    "\n",
    "        if cast_type:\n",
    "            training[feature_name_added] = training[feature_name_added].fillna(-1).astype('uint16')\n",
    "        print('count ip attribution with group by in hist data:', group_by_cols)\n",
    "        feature_name_added1 = '_'.join(group_by_cols) + \"count_attribution_in_hist\"\n",
    "        n_chans = training_hist_attribution[group_by_cols + [counting_col]] \\\n",
    "            .groupby(by=group_by_cols)[[counting_col]] \\\n",
    "            .count().reset_index().rename(columns={counting_col: feature_name_added1 })\n",
    "        training = training.merge(n_chans, on=group_by_cols, how='left')\n",
    "        del n_chans\n",
    "        gc.collect()\n",
    "\n",
    "        \n",
    "        if qcut_count != 0:\n",
    "            print('before qcut', feature_name_added1, training[feature_name_added1].describe())\n",
    "            quantile_cut = training[feature_name_added1].quantile(qcut_count)\n",
    "            training[feature_name_added1] = training[feature_name_added1].apply(lambda x: x if x < quantile_cut else -1)\n",
    "            print('after qcut', feature_name_added1, training[feature_name_added1].describe())\n",
    "            \n",
    "        if cast_type:\n",
    "            training[feature_name_added1] = training[feature_name_added1].fillna(-1).astype('uint16')\n",
    "        #training[feature_name_added1] = training[feature_name_added1].astype('uint16')\n",
    "                            \n",
    "        feature_name_added2 = '_'.join(group_by_cols) + \"count_attribution_rate_in_hist\"\n",
    "        training[feature_name_added2] = \\\n",
    "            training[feature_name_added1] / training[feature_name_added] * 10000.0\n",
    "            \n",
    "        if cast_type:\n",
    "            training[feature_name_added2] = training[feature_name_added2].fillna(-1).astype('uint16')\n",
    "            \n",
    "        features_added.append(feature_name_added)\n",
    "        features_added.append(feature_name_added1)\n",
    "        features_added.append('_'.join(group_by_cols) + \"count_attribution_rate_in_hist\")\n",
    "        \n",
    "    print('added features:', features_added)\n",
    "                                               \n",
    "    return training, features_added\n",
    "def add_statistic_feature1(group_by_cols, training, training_hist, training_hist_attribution, \n",
    "                          with_hist, counting_col='channel'):\n",
    "    features_added = []\n",
    "    feature_name_added = '_'.join(group_by_cols) + 'count'\n",
    "    print('count ip with group by:', group_by_cols)\n",
    "    n_chans = training[group_by_cols + [counting_col]].groupby(by=group_by_cols)[[counting_col]] \\\n",
    "        .count().reset_index().rename(columns={counting_col: feature_name_added})\n",
    "    training = training.merge(n_chans, on=group_by_cols, how='left')\n",
    "    del n_chans\n",
    "    gc.collect()\n",
    "    training[feature_name_added] = training[feature_name_added].astype('uint16')\n",
    "    features_added.append(feature_name_added)\n",
    "    \n",
    "    if with_hist:\n",
    "        print('count ip with group by in hist data:', group_by_cols)\n",
    "        feature_name_added = '_'.join(group_by_cols) + \"count_in_hist\"\n",
    "        n_chans = training_hist[group_by_cols + [counting_col]].groupby(by=group_by_cols)[[counting_col]] \\\n",
    "            .count().reset_index().rename(columns={counting_col: feature_name_added})\n",
    "        training = training.merge(n_chans, on=group_by_cols, how='left')\n",
    "        del n_chans\n",
    "        gc.collect()\n",
    "        #training[feature_name_added] = training[feature_name_added].astype('uint16')\n",
    "        print('count ip attribution with group by in hist data:', group_by_cols)\n",
    "        feature_name_added1 = '_'.join(group_by_cols) + \"count_attribution_in_hist\"\n",
    "        n_chans = training_hist_attribution[group_by_cols + [counting_col]] \\\n",
    "            .groupby(by=group_by_cols)[[counting_col]] \\\n",
    "            .count().reset_index().rename(columns={counting_col: feature_name_added1 })\n",
    "        training = training.merge(n_chans, on=group_by_cols, how='left')\n",
    "        del n_chans\n",
    "        gc.collect()\n",
    "        #training[feature_name_added1] = training[feature_name_added1].astype('uint16')\n",
    "                                               \n",
    "        training['_'.join(group_by_cols) + \"count_attribution_rate_in_hist\"] = \\\n",
    "            training[feature_name_added1] / training[feature_name_added]\n",
    "            \n",
    "        features_added.append(feature_name_added)\n",
    "        features_added.append(feature_name_added1)\n",
    "        features_added.append('_'.join(group_by_cols) + \"count_attribution_rate_in_hist\")\n",
    "        \n",
    "    print('added features:', features_added)\n",
    "                                               \n",
    "    return training, features_added\n",
    "\n",
    "def generate_counting_history_features(data, history, history_attribution, remove_hist_profile_count=0):\n",
    "        \n",
    "    new_features = []\n",
    "    \n",
    "    # Count by IP,DAY,HOUR\n",
    "    print('a given IP address within each hour...')\n",
    "    data, features_added = add_statistic_feature(['ip','day','hour'], data, history, history_attribution, False)\n",
    "    new_features = new_features + features_added\n",
    "    gc.collect()\n",
    "\n",
    "    # Count by IP and APP\n",
    "    data, features_added = add_statistic_feature(['ip','app'], data, history, history_attribution, True)\n",
    "    new_features = new_features + features_added\n",
    "    \n",
    "    # Count by IP and channel\n",
    "    data, features_added = add_statistic_feature(['ip','channel'], data, history, history_attribution, True, counting_col='os')\n",
    "    new_features = new_features + features_added\n",
    "    \n",
    "    # Count by IP and channel app\n",
    "    data, features_added = add_statistic_feature(['ip','channel', 'app'], data, history, history_attribution, True, counting_col='os')\n",
    "    new_features = new_features + features_added\n",
    "    \n",
    "    data, features_added  = add_statistic_feature(['ip','app','os'], data, history, history_attribution, True)\n",
    "    new_features = new_features + features_added\n",
    "\n",
    "    #######\n",
    "    # Count by IP\n",
    "    data, features_added  = add_statistic_feature(['ip'], data, history, history_attribution, True)\n",
    "    new_features = new_features + features_added\n",
    "\n",
    "    # Count by IP HOUR CHANNEL                                               \n",
    "    data, features_added  = add_statistic_feature(['ip','hour','channel'], \\\n",
    "        data, history, history_attribution, True, counting_col='os')\n",
    "    new_features = new_features + features_added\n",
    "\n",
    "    # Count by IP HOUR Device\n",
    "    data, features_added  = add_statistic_feature(['ip','hour','os'], \\\n",
    "        data, history, history_attribution, True)\n",
    "    new_features = new_features + features_added\n",
    "\n",
    "    data, features_added  = add_statistic_feature(['ip','hour','app'], \\\n",
    "        data, history, history_attribution, True)\n",
    "    new_features = new_features + features_added\n",
    "\n",
    "    data, features_added  = add_statistic_feature(['ip','hour','device'], \\\n",
    "        data, history, history_attribution, True)\n",
    "    new_features = new_features + features_added\n",
    "    \n",
    "    if remove_hist_profile_count != 0:\n",
    "        data = data.query('ipcount_in_hist > {}'.format(remove_hist_profile_count))\n",
    "    \n",
    "    return data, new_features\n",
    "\n",
    "\n",
    "for_test1 = False\n",
    "\n",
    "if for_test1:\n",
    "\n",
    "    lgb_model = lgb.Booster(model_file='model1.txt')\n",
    "    #prepare test data:\n",
    "    train = pd.read_csv(path_train, dtype=dtypes,\n",
    "            header=0,usecols=train_cols,parse_dates=[\"click_time\"])#.sample(1000)\n",
    "    test = pd.read_csv(path_test, dtype=dtypes, header=0,\n",
    "            usecols=test_cols,parse_dates=[\"click_time\"])#.sample(1000)\n",
    "    train=train.append(test)\n",
    "    del test\n",
    "    gc.collect()\n",
    "    print(\"Creating new time features in train: 'hour' and 'day'...\")\n",
    "    train['hour'] = train[\"click_time\"].dt.hour.astype('uint8')\n",
    "    train['day'] = train[\"click_time\"].dt.day.astype('uint8')\n",
    "    \n",
    "    train, train_ip_contains_training_day, train_ip_contains_training_day_attributed = \\\n",
    "        prepare_data(train, 10, 3, 1, only_for_ip_with_hist=True)\n",
    "\n",
    "    train, new_features = generate_counting_history_features(train, train_ip_contains_training_day, \n",
    "                                                             train_ip_contains_training_day_attributed, 4)\n",
    "\n",
    "    print('test data:', train)\n",
    "    print('new features:', new_features)\n",
    "    print(\"Preparing data for submission...\")\n",
    "\n",
    "    submit = pd.read_csv(path_test, dtype='int', usecols=['click_id'])\n",
    "    print('submit test len:', len(submit))\n",
    "  \n",
    "\n",
    "    categorical = ['app', 'device', 'os', 'channel', 'hour']\n",
    "\n",
    "    predictors1 = categorical + new_features\n",
    "    print(\"Predicting the submission data...\")\n",
    "    submit['is_attributed'] = lgb_model.predict(train[predictors1], num_iteration=lgb_model.best_iteration)\n",
    "\n",
    "    print(\"Writing the submission data into a csv file...\")\n",
    "\n",
    "    submit.to_csv(\"submission.csv\",index=False)\n",
    "\n",
    "    print(\"All done...\")\n",
    "    \n",
    "    import sys\n",
    "    sys.exit(0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "2e57bfc0-e2e4-43a6-8e1c-20fa9a7d3c8d",
    "_uuid": "a74b9eaa05283c2e2d048455fe298507656ca6e7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The initial size of the train set is 184903890\n",
      "The initial size of the test set is 18790469\n",
      "Binding the training and test set together...\n",
      "Creating new time features in train: 'hour' and 'day'...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#skiprows=skip, \n",
    "train = pd.read_csv(path_train, dtype=dtypes,\n",
    "        header=0,usecols=train_cols,parse_dates=[\"click_time\"])#.sample(1000)\n",
    "test = pd.read_csv(path_test, dtype=dtypes, header=0,\n",
    "        usecols=test_cols,parse_dates=[\"click_time\"])#.sample(1000)\n",
    "#test['is_attributed'] = -1\n",
    "\n",
    "len_train = len(train)\n",
    "print('The initial size of the train set is', len_train)\n",
    "print('The initial size of the test set is', len(test))\n",
    "print('Binding the training and test set together...')\n",
    "\n",
    "\n",
    "\n",
    "print(\"Creating new time features in train: 'hour' and 'day'...\")\n",
    "train['hour'] = train[\"click_time\"].dt.hour.astype('uint8')\n",
    "train['day'] = train[\"click_time\"].dt.day.astype('uint8')\n",
    "\n",
    "\n",
    "print(\"Creating new time features in test: 'hour' and 'day'...\")\n",
    "test['hour'] = test[\"click_time\"].dt.hour.astype('uint8')\n",
    "test['day'] = test[\"click_time\"].dt.day.astype('uint8')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "3a46577f-c98e-4c7c-9451-cf6809ed74e7",
    "_uuid": "fa0491bc2ffe926edc61206a5d6eaaf06cde247f",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#test['hour'] = test[\"click_time\"].dt.hour.astype('uint8')\n",
    "#test['day'] = test[\"click_time\"].dt.day.astype('uint8')\n",
    "\n",
    "train, train_ip_contains_training_day, train_ip_contains_training_day_attributed = \\\n",
    "    prepare_data(train, 9, 3, 4, only_for_ip_with_hist=True)\n",
    "\n",
    "train, new_features = generate_counting_history_features(train, train_ip_contains_training_day, \n",
    "                                                         train_ip_contains_training_day_attributed, 4)\n",
    "\n",
    "print('train data:', train)\n",
    "print('new features:', new_features)\n",
    "\n",
    "val = train.set_index('ip').loc[lambda x: (x.index) % 17 == 0].reset_index()\n",
    "print(val)\n",
    "print('The size of the validation set is ', len(val))\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "train = train.set_index('ip').loc[lambda x: (x.index) % 17 != 0].reset_index()\n",
    "print('The size of the train set is ', len(train))\n",
    "\n",
    "target = 'is_attributed'\n",
    "train[target] = train[target].astype('uint8')\n",
    "train.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "predictors0 = ['device', 'app', 'os', 'channel', 'hour', # Starter Vars, Then new features below\n",
    "              'ip_day_hourcount','ipcount','ip_appcount', 'ip_app_oscount',\n",
    "              \"ip_hour_channelcount\", \"ip_hour_oscount\", \"ip_hour_appcount\",\"ip_hour_devicecount\"]\n",
    "\n",
    "categorical = ['app', 'device', 'os', 'channel', 'hour']\n",
    "\n",
    "predictors1 = categorical + new_features\n",
    "#for ii in new_features:\n",
    "#    predictors1 = predictors1 + ii\n",
    "#print(predictors1)\n",
    "gc.collect()\n",
    "\n",
    "#train.fillna(value={x:-1 for x in new_features})\n",
    "\n",
    "print(\"Preparing the datasets for training...\")\n",
    "\n",
    "params = {\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'binary',\n",
    "    'metric': 'auc',\n",
    "    'learning_rate': 0.1,\n",
    "    'num_leaves': 7,  \n",
    "    'max_depth': 4,  \n",
    "    'min_child_samples': 100,  \n",
    "    'max_bin': 150,  \n",
    "    'subsample': 0.7,  \n",
    "    'subsample_freq': 1,  \n",
    "    'colsample_bytree': 0.7,  \n",
    "    'min_child_weight': 0,  \n",
    "    'subsample_for_bin': 200000,  \n",
    "    'min_split_gain': 0,  \n",
    "    'reg_alpha': 0,  \n",
    "    'reg_lambda': 0,  \n",
    "    'nthread': 5,\n",
    "    'verbose': 9,\n",
    "    #'is_unbalance': True,\n",
    "    'scale_pos_weight':99 \n",
    "    }\n",
    "    \n",
    "predictors_to_train = [predictors1]\n",
    "\n",
    "for predictors in predictors_to_train:\n",
    "    print('training with :', predictors)\n",
    "    #print('training data: ', train[predictors].values)\n",
    "    #print('validation data: ', val[predictors].values)\n",
    "    dtrain = lgb.Dataset(train[predictors].values, label=train[target].values,\n",
    "                          feature_name=predictors,\n",
    "                          categorical_feature=categorical\n",
    "                          )\n",
    "    dvalid = lgb.Dataset(val[predictors].values, label=val[target].values,\n",
    "                          feature_name=predictors,\n",
    "                          categorical_feature=categorical\n",
    "                          )\n",
    "\n",
    "    evals_results = {}\n",
    "    print(\"Training the model...\")\n",
    "\n",
    "    lgb_model = lgb.train(params, \n",
    "                     dtrain, \n",
    "                     valid_sets=[dtrain, dvalid], \n",
    "                     valid_names=['train','valid'], \n",
    "                     evals_result=evals_results, \n",
    "                     num_boost_round=1000,\n",
    "                     early_stopping_rounds=30,\n",
    "                     verbose_eval=50, \n",
    "                     feval=None)\n",
    "\n",
    "    #del train\n",
    "    #del val\n",
    "    #gc.collect()\n",
    "\n",
    "    # Nick's Feature Importance Plot\n",
    "    import matplotlib.pyplot as plt\n",
    "    f, ax = plt.subplots(figsize=[7,10])\n",
    "    lgb.plot_importance(lgb_model, ax=ax, max_num_features=len(predictors))\n",
    "    plt.title(\"Light GBM Feature Importance\")\n",
    "    plt.savefig('feature_import.png')\n",
    "\n",
    "    # Feature names:\n",
    "    print('Feature names:', lgb_model.feature_name())\n",
    "    # Feature importances:\n",
    "    print('Feature importances:', list(lgb_model.feature_importance()))\n",
    "\n",
    "    feature_imp = pd.DataFrame(lgb_model.feature_name(),list(lgb_model.feature_importance()))\n",
    "    \n",
    "    print('saving model...')\n",
    "    lgb_model.save_model('model.txt')\n",
    "    \n",
    "    print('model saved')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "952d1edf-174f-4048-92ac-7d5115c2f1a6",
    "_uuid": "73214a3ae8e59ca6b1c122e6d31c1284364c8eb4",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#persist training data\n",
    "persist_data = False\n",
    "if persist_data:\n",
    "    train.to_csv(\"train.csv.gzip\",index=False,compression='gzip')\n",
    "    val.to_csv(\"val.csv.gzip\",index=False,compression='gzip')\n",
    "\n",
    "    import pickle\n",
    "    pickle.dump(predictors1, open('predictors1.pickle','wb'))\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for_test = False\n",
    "\n",
    "if for_test:\n",
    "    #del train\n",
    "    #del test\n",
    "    del train_ip_contains_training_day\n",
    "    del train_ip_contains_training_day_attributed\n",
    "    del val\n",
    "    gc.collect()\n",
    "\n",
    "    #prepare test data:\n",
    "    train = pd.read_csv(path_train, dtype=dtypes,\n",
    "            header=0,usecols=train_cols,parse_dates=[\"click_time\"])#.sample(1000)\n",
    "    test = pd.read_csv(path_test, dtype=dtypes, header=0,\n",
    "            usecols=test_cols,parse_dates=[\"click_time\"])#.sample(1000)\n",
    "    train=train.append(test)\n",
    "    del test\n",
    "    gc.collect()\n",
    "    print(\"Creating new time features in train: 'hour' and 'day'...\")\n",
    "    train['hour'] = train[\"click_time\"].dt.hour.astype('uint8')\n",
    "    train['day'] = train[\"click_time\"].dt.day.astype('uint8')\n",
    "    \n",
    "    train, train_ip_contains_training_day, train_ip_contains_training_day_attributed = \\\n",
    "        prepare_data(train, 10, 3, 1, only_for_ip_with_hist=True)\n",
    "\n",
    "    train, new_features = generate_counting_history_features(train, train_ip_contains_training_day, \n",
    "                                                             train_ip_contains_training_day_attributed, 4)\n",
    "\n",
    "    print('test data:', train)\n",
    "    print('new features:', new_features)\n",
    "    print(\"Preparing data for submission...\")\n",
    "\n",
    "    submit = pd.read_csv(path_test, dtype='int', usecols=['click_id'])\n",
    "    print('submit test len:', len(submit))\n",
    "    print(\"Predicting the submission data...\")\n",
    "    submit['is_attributed'] = lgb_model.predict(train[predictors1], num_iteration=lgb_model.best_iteration)\n",
    "\n",
    "    print(\"Writing the submission data into a csv file...\")\n",
    "\n",
    "    submit.to_csv(\"submission.csv\",index=False)\n",
    "\n",
    "    print(\"All done...\")\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "76224c53-eeaf-408f-9591-846eb0b22157",
    "_uuid": "88d268bb4880465a08872ffd711cb41891ddd1a2",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "load_persist = False\n",
    "if load_persist:\n",
    "    print('reading train.csv.gzip')\n",
    "\n",
    "    import numpy as np # linear algebra\n",
    "    import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "    train = pd.read_csv('train.csv.gzip',compression='gzip')\n",
    "\n",
    "    print('read done')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "83eb5aeb-aaf6-471b-82ef-b13f6b63909c",
    "_uuid": "84a23892f3e9a731740ff0d143c53ea40b7bc031",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#print(train.groupby('ip_channel_appcount_attribution_rate_in_hist')['ip'].count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "9aed49d4-c504-470c-8af1-15e77db07a62",
    "_uuid": "3872f4faab6ccdac83dd3c0326ab7b9b2d437a3b",
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "Another CTR comp and so i suspect libffm will play its part, after all it is an atomic bomb for this kind of stuff.\n",
    "A sci-kit learn inspired script to convert pandas dataframes into libFFM style data.\n",
    "\n",
    "The script is fairly hacky (hey thats Kaggle) and takes a little while to run a huge dataset.\n",
    "The key to using this class is setting up the features dtypes correctly for output (ammend transform to suit your needs)\n",
    "\n",
    "Example below\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "class FFMFormatPandas:\n",
    "    def __init__(self):\n",
    "        self.field_index_ = None\n",
    "        self.feature_index_ = None\n",
    "        self.y = None\n",
    "\n",
    "    def fit(self, df, y=None):\n",
    "        self.y = y\n",
    "        df_ffm = df[df.columns.difference([self.y])]\n",
    "        if self.field_index_ is None:\n",
    "            self.field_index_ = {col: i for i, col in enumerate(df_ffm)}\n",
    "\n",
    "        if self.feature_index_ is not None:\n",
    "            last_idx = max(list(self.feature_index_.values()))\n",
    "\n",
    "        if self.feature_index_ is None:\n",
    "            self.feature_index_ = dict()\n",
    "            last_idx = 0\n",
    "\n",
    "        for col in df.columns:\n",
    "            vals = df[col].unique()\n",
    "            for val in vals:\n",
    "                if pd.isnull(val):\n",
    "                    continue\n",
    "                name = '{}_{}'.format(col, val)\n",
    "                if name not in self.feature_index_:\n",
    "                    self.feature_index_[name] = last_idx\n",
    "                    last_idx += 1\n",
    "            self.feature_index_[col] = last_idx\n",
    "            last_idx += 1\n",
    "        return self\n",
    "\n",
    "    def fit_transform(self, df, y=None):\n",
    "        self.fit(df, y)\n",
    "        return self.transform(df)\n",
    "\n",
    "    def transform_row_(self, row, t):\n",
    "        ffm = []\n",
    "        if self.y != None:\n",
    "            ffm.append(str(row.loc[row.index == self.y][0]))\n",
    "        if self.y is None:\n",
    "            ffm.append(str(0))\n",
    "\n",
    "        for col, val in row.loc[row.index != self.y].to_dict().items():\n",
    "            col_type = t[col]\n",
    "            name = '{}_{}'.format(col, val)\n",
    "            if col_type.kind ==  'O':\n",
    "                ffm.append('{}:{}:1'.format(self.field_index_[col], self.feature_index_[name]))\n",
    "            elif col_type.kind == 'i':\n",
    "                ffm.append('{}:{}:{}'.format(self.field_index_[col], self.feature_index_[col], val))\n",
    "        return ' '.join(ffm)\n",
    "\n",
    "    def transform(self, df):\n",
    "        t = df.dtypes.to_dict()\n",
    "        return pd.Series({idx: self.transform_row_(row, t) for idx, row in df.iterrows()})\n",
    "\n",
    "use_FFM = False\n",
    "\n",
    "if use_FFM:\n",
    "    ffm_train = FFMFormatPandas()\n",
    "    ffm_train_data = ffm_train.fit_transform(train, y='is_attributed')\n",
    "\n",
    "    print('FFM data:',ffm_train_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
