{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c06847a6bbb76958423b979874a31d877dfce7c8",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "import os\n",
    "print(os.listdir(\"../input\"))\n",
    "\n",
    "# Any results you write to the current directory are saved as output.\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import lightgbm as lgb\n",
    "import gc\n",
    "\n",
    "\n",
    "path = '../input/' \n",
    "path_train = path + 'train.csv'\n",
    "path_test = path + 'test.csv'\n",
    "\n",
    "train_cols = ['ip', 'app', 'device', 'os', 'channel', 'click_time', 'is_attributed']\n",
    "test_cols = ['ip', 'app', 'device', 'os', 'channel', 'click_time']\n",
    "\n",
    "dtypes = {\n",
    "        'ip'            : 'uint32',\n",
    "        'app'           : 'uint16',\n",
    "        'device'        : 'uint16',\n",
    "        'os'            : 'uint16',\n",
    "        'channel'       : 'uint16',\n",
    "        'is_attributed' : 'uint8',\n",
    "        'click_id'      : 'uint32'\n",
    "        }\n",
    "        \n",
    "skip = range(1, 140000000)\n",
    "print(\"Loading Data\")\n",
    "#skiprows=skip, \n",
    "train = pd.read_csv(path_train, dtype=dtypes,\n",
    "        header=0,usecols=train_cols,parse_dates=[\"click_time\"])#.sample(1000)\n",
    "test = pd.read_csv(path_test, dtype=dtypes, header=0,\n",
    "        usecols=test_cols,parse_dates=[\"click_time\"])#.sample(1000)\n",
    "#test['is_attributed'] = -1\n",
    "\n",
    "len_train = len(train)\n",
    "print('The initial size of the train set is', len_train)\n",
    "print('The initial size of the test set is', len(test))\n",
    "print('Binding the training and test set together...')\n",
    "\n",
    "\n",
    "\n",
    "print(\"Creating new time features in train: 'hour' and 'day'...\")\n",
    "train['hour'] = train[\"click_time\"].dt.hour.astype('uint8')\n",
    "train['day'] = train[\"click_time\"].dt.day.astype('uint8')\n",
    "\n",
    "\n",
    "print(\"Creating new time features in test: 'hour' and 'day'...\")\n",
    "test['hour'] = test[\"click_time\"].dt.hour.astype('uint8')\n",
    "test['day'] = test[\"click_time\"].dt.day.astype('uint8')\n",
    "\n",
    "\n",
    "def prepare_data(data, training_day, profile_days, sample_count=1):\n",
    "    if sample_count != 1:\n",
    "        #sample 1/4 of the data:\n",
    "        data = data.set_index('ip').loc[lambda x: (x.index + 401) % sample_count == 0].reset_index()\n",
    "        len_train = len(data)\n",
    "        print('len after sample:', len_train)\n",
    "\n",
    "    train_ip_contains_training_day = data.groupby('ip').filter(lambda x: x['day'].max() == training_day)\n",
    "\n",
    "    print('train_ip_contains_training_day', train_ip_contains_training_day)\n",
    "    print('train_ip_contains_training_day unique ips:', len(train_ip_contains_training_day['ip'].unique()))\n",
    "\n",
    "    train_ip_contains_training_day = train_ip_contains_training_day \\\n",
    "        .query('day < {0} & day > {1}'.format(training_day, training_day - 1 - profile_days) )\n",
    "    print('train_ip_contains_training_day unique ips:', len(train_ip_contains_training_day['ip'].unique()))\n",
    "\n",
    "    print('split attributed data:')\n",
    "    train_ip_contains_training_day_attributed = train_ip_contains_training_day.query('is_attributed == 1')\n",
    "    print('len:',len(train_ip_contains_training_day_attributed))\n",
    "\n",
    "    #only use data on 9 to train, but data before 9 as features\n",
    "    train = data.query('day == {}'.format(training_day))\n",
    "    print('training data len:', len(train))\n",
    "    \n",
    "    return train, train_ip_contains_training_day, train_ip_contains_training_day_attributed\n",
    "\n",
    "def add_statistic_feature(group_by_cols, training, training_hist, training_hist_attribution, \n",
    "                          with_hist, counting_col='channel'):\n",
    "    features_added = []\n",
    "    feature_name_added = '_'.join(group_by_cols) + 'count'\n",
    "    print('count ip with group by:', group_by_cols)\n",
    "    n_chans = training[group_by_cols + [counting_col]].groupby(by=group_by_cols)[[counting_col]] \\\n",
    "        .count().reset_index().rename(columns={counting_col: feature_name_added})\n",
    "    training = training.merge(n_chans, on=group_by_cols, how='left')\n",
    "    del n_chans\n",
    "    gc.collect()\n",
    "    training[feature_name_added] = training[feature_name_added].astype('uint16')\n",
    "    features_added.append(feature_name_added)\n",
    "    \n",
    "    if with_hist:\n",
    "        print('count ip with group by in hist data:', group_by_cols)\n",
    "        feature_name_added = '_'.join(group_by_cols) + \"count_in_hist\"\n",
    "        n_chans = training_hist[group_by_cols + [counting_col]].groupby(by=group_by_cols)[[counting_col]] \\\n",
    "            .count().reset_index().rename(columns={counting_col: feature_name_added})\n",
    "        training = training.merge(n_chans, on=group_by_cols, how='left')\n",
    "        del n_chans\n",
    "        gc.collect()\n",
    "        #training[feature_name_added] = training[feature_name_added].astype('uint16')\n",
    "        print('count ip attribution with group by in hist data:', group_by_cols)\n",
    "        feature_name_added1 = '_'.join(group_by_cols) + \"count_attribution_in_hist\"\n",
    "        n_chans = training_hist_attribution[group_by_cols + [counting_col]] \\\n",
    "            .groupby(by=group_by_cols)[[counting_col]] \\\n",
    "            .count().reset_index().rename(columns={counting_col: feature_name_added1 })\n",
    "        training = training.merge(n_chans, on=group_by_cols, how='left')\n",
    "        del n_chans\n",
    "        gc.collect()\n",
    "        #training[feature_name_added1] = training[feature_name_added1].astype('uint16')\n",
    "                                               \n",
    "        training['_'.join(group_by_cols) + \"count_attribution_rate_in_hist\"] = \\\n",
    "            training[feature_name_added] / training[feature_name_added1]\n",
    "            \n",
    "        features_added.append(feature_name_added)\n",
    "        features_added.append(feature_name_added1)\n",
    "        features_added.append('_'.join(group_by_cols) + \"count_attribution_rate_in_hist\")\n",
    "        \n",
    "    print('added features:', features_added)\n",
    "                                               \n",
    "    return training, features_added\n",
    "\n",
    "def generate_counting_history_features(data, history, history_attribution):\n",
    "        \n",
    "    new_features = []\n",
    "\n",
    "    # Count by IP,DAY,HOUR\n",
    "    print('a given IP address within each hour...')\n",
    "    data, features_added = add_statistic_feature(['ip','day','hour'], data, history, history_attribution, False)\n",
    "    new_features = new_features + features_added\n",
    "    gc.collect()\n",
    "\n",
    "    # Count by IP and APP\n",
    "    data, features_added = add_statistic_feature(['ip','app'], data, history, history_attribution, True)\n",
    "    new_features = new_features + features_added\n",
    "    \n",
    "    data, features_added  = add_statistic_feature(['ip','app','os'], data, history, history_attribution, True)\n",
    "    new_features = new_features + features_added\n",
    "\n",
    "    #######\n",
    "    # Count by IP\n",
    "    data, features_added  = add_statistic_feature(['ip'], data, history, history_attribution, True)\n",
    "    new_features = new_features + features_added\n",
    "\n",
    "    # Count by IP HOUR CHANNEL                                               \n",
    "    data, features_added  = add_statistic_feature(['ip','hour','channel'], \\\n",
    "        data, history, history_attribution, True, counting_col='os')\n",
    "    new_features = new_features + features_added\n",
    "\n",
    "    # Count by IP HOUR Device\n",
    "    data, features_added  = add_statistic_feature(['ip','hour','os'], \\\n",
    "        data, history, history_attribution, True)\n",
    "    new_features = new_features + features_added\n",
    "\n",
    "    data, features_added  = add_statistic_feature(['ip','hour','app'], \\\n",
    "        data, history, history_attribution, True)\n",
    "    new_features = new_features + features_added\n",
    "\n",
    "    data, features_added  = add_statistic_feature(['ip','hour','device'], \\\n",
    "        data, history, history_attribution, True)\n",
    "    new_features = new_features + features_added\n",
    "    \n",
    "    return data, new_features\n",
    "\n",
    "#test['hour'] = test[\"click_time\"].dt.hour.astype('uint8')\n",
    "#test['day'] = test[\"click_time\"].dt.day.astype('uint8')\n",
    "\n",
    "train, new_features = generate_counting_history_features(train, train_ip_contains_9, train_ip_contains_9_attributed)\n",
    "\n",
    "print('train data:', train)\n",
    "print('new features:', new_features)\n",
    "\n",
    "val = train.set_index('ip').loc[lambda x: (x.index) % 17 == 0].reset_index()\n",
    "print(val)\n",
    "print('The size of the validation set is ', len(val))\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "train = train.set_index('ip').loc[lambda x: (x.index) % 17 != 0].reset_index()\n",
    "print('The size of the train set is ', len(train))\n",
    "\n",
    "target = 'is_attributed'\n",
    "train[target] = train[target].astype('uint8')\n",
    "train.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing the datasets for training...\n",
      "training with : ['app', 'device', 'os', 'channel', 'hour', 'ip_day_hourcount', 'ip_appcount', 'ip_appcount_in_hist', 'ip_appcount_attribution_in_hist', 'ip_appcount_attribution_rate_in_hist', 'ip_app_oscount', 'ip_app_oscount_in_hist', 'ip_app_oscount_attribution_in_hist', 'ip_app_oscount_attribution_rate_in_hist', 'ipcount', 'ipcount_in_hist', 'ipcount_attribution_in_hist', 'ipcount_attribution_rate_in_hist', 'ip_hour_channelcount', 'ip_hour_channelcount_in_hist', 'ip_hour_channelcount_attribution_in_hist', 'ip_hour_channelcount_attribution_rate_in_hist', 'ip_hour_oscount', 'ip_hour_oscount_in_hist', 'ip_hour_oscount_attribution_in_hist', 'ip_hour_oscount_attribution_rate_in_hist', 'ip_hour_appcount', 'ip_hour_appcount_in_hist', 'ip_hour_appcount_attribution_in_hist', 'ip_hour_appcount_attribution_rate_in_hist', 'ip_hour_devicecount', 'ip_hour_devicecount_in_hist', 'ip_hour_devicecount_attribution_in_hist', 'ip_hour_devicecount_attribution_rate_in_hist']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "predictors0 = ['device', 'app', 'os', 'channel', 'hour', # Starter Vars, Then new features below\n",
    "              'ip_day_hourcount','ipcount','ip_appcount', 'ip_app_oscount',\n",
    "              \"ip_hour_channelcount\", \"ip_hour_oscount\", \"ip_hour_appcount\",\"ip_hour_devicecount\"]\n",
    "\n",
    "categorical = ['app', 'device', 'os', 'channel', 'hour']\n",
    "\n",
    "predictors1 = categorical + new_features\n",
    "#for ii in new_features:\n",
    "#    predictors1 = predictors1 + ii\n",
    "#print(predictors1)\n",
    "gc.collect()\n",
    "\n",
    "#train.fillna(value={x:-1 for x in new_features})\n",
    "\n",
    "print(\"Preparing the datasets for training...\")\n",
    "\n",
    "params = {\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'binary',\n",
    "    'metric': 'auc',\n",
    "    'learning_rate': 0.1,\n",
    "    'num_leaves': 7,  \n",
    "    'max_depth': 4,  \n",
    "    'min_child_samples': 100,  \n",
    "    'max_bin': 150,  \n",
    "    'subsample': 0.7,  \n",
    "    'subsample_freq': 1,  \n",
    "    'colsample_bytree': 0.7,  \n",
    "    'min_child_weight': 0,  \n",
    "    'subsample_for_bin': 200000,  \n",
    "    'min_split_gain': 0,  \n",
    "    'reg_alpha': 0,  \n",
    "    'reg_lambda': 0,  \n",
    "    'nthread': 5,\n",
    "    'verbose': 9,\n",
    "    #'is_unbalance': True,\n",
    "    'scale_pos_weight':99 \n",
    "    }\n",
    "    \n",
    "predictors_to_train = [predictors1]\n",
    "\n",
    "for predictors in predictors_to_train:\n",
    "    print('training with :', predictors)\n",
    "    #print('training data: ', train[predictors].values)\n",
    "    #print('validation data: ', val[predictors].values)\n",
    "    dtrain = lgb.Dataset(train[predictors].values, label=train[target].values,\n",
    "                          feature_name=predictors,\n",
    "                          categorical_feature=categorical\n",
    "                          )\n",
    "    dvalid = lgb.Dataset(val[predictors].values, label=val[target].values,\n",
    "                          feature_name=predictors,\n",
    "                          categorical_feature=categorical\n",
    "                          )\n",
    "\n",
    "    evals_results = {}\n",
    "    print(\"Training the model...\")\n",
    "\n",
    "    lgb_model = lgb.train(params, \n",
    "                     dtrain, \n",
    "                     valid_sets=[dtrain, dvalid], \n",
    "                     valid_names=['train','valid'], \n",
    "                     evals_result=evals_results, \n",
    "                     num_boost_round=1000,\n",
    "                     early_stopping_rounds=30,\n",
    "                     verbose_eval=50, \n",
    "                     feval=None)\n",
    "\n",
    "    #del train\n",
    "    #del val\n",
    "    #gc.collect()\n",
    "\n",
    "    # Nick's Feature Importance Plot\n",
    "    import matplotlib.pyplot as plt\n",
    "    f, ax = plt.subplots(figsize=[7,10])\n",
    "    lgb.plot_importance(lgb_model, ax=ax, max_num_features=len(predictors))\n",
    "    plt.title(\"Light GBM Feature Importance\")\n",
    "    plt.savefig('feature_import.png')\n",
    "\n",
    "    # Feature names:\n",
    "    print('Feature names:', lgb_model.feature_name())\n",
    "    # Feature importances:\n",
    "    print('Feature importances:', list(lgb_model.feature_importance()))\n",
    "\n",
    "    feature_imp = pd.DataFrame(lgb_model.feature_name(),list(lgb_model.feature_importance()))\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
