arc testing:

base @ 13:
	1/6 train, val sample, use all history feature, label 4:00-15:00, with casting uint16,
	qcut 0.98,scale_pos_weight:99.7

base @20:
	1/6 train, val sample, use all history feature, label 4:00-15:00, with casting uint16,
	qcut 0.98,scale_pos_weight:99.0

1. feature and label use only range from 4:00 - 15:00, with casting to uint16:
    1. train's auc: 0.982385	valid's auc: 0.966781
2. use all history feature, label 4:00-15:00, with casting to uint16:
    1. train's auc: 0.981373	valid's auc: 0.968077
    2. mac:train's auc: 0.981373	valid's auc: 0.968077
3. use all history feature, label 4:00-15:00, without casting to uint16:
    1. train's auc: 0.981373	valid's auc: 0.968077
4. use all history feature, label 4:00-15:00, without casting to uint16, qcut 0.95:
    1. kernel:train's auc: 0.982129	valid's auc: 0.968454
5. use all history feature, label 4:00-15:00, without casting to uint16, qcut 0.98:
    1. kernel:train's auc: 0.983909	valid's auc: 0.96873
6. use all history feature, label 4:00-15:00, without casting to uint16, qcut 0.95, discretization 500:
    1. mac: train's auc: 0.983119	valid's auc: 0.968293
7. use all history feature, label 4:00-15:00, without casting to uint16, qcut 0.95, scale_pos_weight:99.7:
    1. grid test kernel: train's auc: 0.983002	valid's auc: 0.968661, to merge to base
8. use all history feature, label 4:00-15:00, without casting to uint16, qcut 0.98, add channel, app  os count feature:
    1. grid test 2 kernel: train's auc: 0.982419	valid's auc: 0.968044
9. use all history feature, label 4:00-15:00, with casting to uint16, qcut 0.99:
    1. new val kernel: train's auc: 0.978534	valid's auc: 0.967561
10. use all history feature, label 4:00-15:00, with casting to uint16, qcut 0.98:
    1. new val kernel: train's auc: 0.978534	valid's auc: 0.967561
11. use all history feature, label 4:00-15:00, without casting to uint16, qcut 0.98, add channel count feature:
    1. new val split: train's auc: 0.980404	valid's auc: 0.968002
12. use all history feature, label 4:00-15:00, without casting to uint16, qcut 0.98, add  app count feature:
    1. grid test 2: train's auc: 0.982621	valid's auc: 0.967913
13. use all history feature, label 4:00-15:00, with casting to uint16, qcut 0.98, scale_pos_weight:99.7
    1. new val split: train's auc: 0.982792	valid's auc: 0.968499
14. use all history feature, label 4:00-15:00, with casting to uint16, qcut 0.98, scale_pos_weight:99.3
    1. grid test 2: train's auc: 0.982592	valid's auc: 0.968233
15. use all history feature, label 4:00-15:00, with casting to uint16, qcut 0.98, scale_pos_weight:98.5
    1. new val split: train's auc: 0.983195	valid's auc: 0.968301
16. use all history feature, label 4:00-15:00, with casting to uint16, qcut 0.98, scale_pos_weight:99.9
    1. grid test: train's auc: 0.981288	valid's auc: 0.968207
17. use all history feature, label 4:00-15:00, with casting to uint16, qcut 0.98, scale_pos_weight:98.8
    1. grid test: train's auc: 0.983375	valid's auc: 0.96822
18. use all history feature, label 4:00-15:00, with casting to uint16, qcut 0.98, scale_pos_weight:98.9
    1. grid test 2: train's auc: 0.980357	valid's auc: 0.96775
19. use all history feature, label 4:00-15:00, with casting to uint16, qcut 0.98, scale_pos_weight:99.1
    1. new val split: train's auc: 0.981529	valid's auc: 0.968157
20. use all history feature, label 4:00-15:00, with casting to uint16, qcut 0.98, scale_pos_weight:99.0
    1. grid test: train's auc: 0.983909	valid's auc: 0.96873
    2. merge to base
21. use all history feature, label 4:00-15:00, without casting to uint16, qcut 0.98, scale_pos_weight:99.7
    1. new val split: train's auc: 0.981529	valid's auc: 0.968157
22. use all history feature, label 4:00-15:00, with casting to uint16, qcut 0.95, scale_pos_weight:99.7
    1. grid test: train's auc: 0.983002	valid's auc: 0.968661
23. base + add frequent/infrequent test hours features:
    1. grid test: train's auc: 0.983929	valid's auc: 0.968495
24. base + only the current day's data to extract features
    1. grid test: train's auc: 0.983696	valid's auc: 0.969374
25. base + only the current day's data to extract features + 2 times more data(1/3)
    1. grid test2: train's auc: 0.98291	valid's auc: 0.97013
26. base + only the current day's data to extract features + all training data
    1. new val split: train's auc: 0.98287	valid's auc: 0.972121
    2. new val split: train's auc: 0.98287	valid's auc: 0.972121
27. base + only the current day's data to extract features + all training data + use 09 to train
    1. aliyun:  train's auc: 0.97724    valid's auc: 0.97743     LB: 0.9693,
        submission_notebook.csv.04-04-2018_03-33-26@aliyun
28. base + remove last two features
    1. aliyun: train's auc: 0.982993	valid's auc: 0.968284
29. base + only the current day's data to extract features + all training data + hh feature
    1. grid test2: train's auc: 0.982993	valid's auc: 0.970014
30. base + only the current day's data to extract features 1/6 + hh feature
    1. grid test: 0.983704	valid's auc: 0.969496
31. base + filter 3 channels
    1. grid test: train's auc: 0.853195	valid's auc: 0.799714
    2. grid test: submission_notebook.csv.04-04-2018_03-34-54
32. base + filter 3 channels + hh feature + predict
    1. grid test: train's auc: 0.831272	valid's auc: 0.795663,
        submission_notebook.csv.04-04-2018_01-35-01
33. base + filter 2 apps(3,9) + predict
    1. grid test 2: train's auc: 0.867201	valid's auc: 0.86065
    2. grid test 2: predict
        submission_notebook.csv.04-04-2018_04-41-45
34. base + filter 1 apps(12) + predict
    1. new val split: train's auc: 0.807762	valid's auc: 0.752095
        submission_notebook.csv.04-04-2018_13-30-39
35. base + filter 2 apps(18 14) + predict
    1. grid test 2: train's auc: 0.846225	valid's auc: 0.778439
        submission_notebook.csv.04-04-2018_11-12-10
36. ensemble of 27+31+33: LB 0.9694
37. base + filter 2 apps(8 11) + predict
    1. grid test : train's auc: 0.770682	valid's auc: 0.605115
        submission_notebook.csv.04-04-2018_12-58-05
        ^^^^^^^^^^^^  there's a bug to this versoin deleted
38. base + different lgbm hyperparameters: # num_leaves : 7  ->  9, max_depth  :  4  ->  5, subsample  : 0.7 -> 0.9
    1. grid test 2: train's auc: 0.978664	valid's auc: 0.97764 LB:0.9696
        submission_notebook.csv.04-04-2018_15-08-53
39. base + filter 2 apps(8 11) + predict + different lgbm hyperparameters:
        # num_leaves : 7  ->  9, max_depth  :  4  ->  5, subsample  : 0.7 -> 0.9
    1. grid test :train's auc: 0.832043	valid's auc: 0.818861
        submission_notebook.csv.04-04-2018_16-23-14
40. base + filter 1 apps(12) + predict + different lgbm hyperparameters:
        # num_leaves : 7  ->  9, max_depth  :  4  ->  5, subsample  : 0.7 -> 0.9
    1. grid test 2: train's auc: 0.866256	valid's auc: 0.757562
        submission_notebook.csv.04-04-2018_16-27-49
41. ensemble of models:
    a, submission_notebook.csv.04-04-2018_03-33-26, 27, all training data, params old
    b, submission_notebook.csv.04-04-2018_03-34-54, 31, 3 channels
    c, submission_notebook.csv.04-04-2018_04-41-45, 33, filter 2 apps:3 9
    d, submission_notebook.csv.04-04-2018_11-12-10, 35, filter 2 apps:18 14
    e, submission_notebook.csv.04-04-2018_13-30-39, 34, filter 1 app:12
    f, submission_notebook.csv.04-04-2018_15-08-53, 38, all training data, new params
    LB: 0.9697
42, FFM of full training data:
    1. aliyun: very bad, because of too big cardinality, need to qcut to 1000
43, FFM of full training data, qcut 100, same qcut bins with training for val and test,
    remove ip count to reduce overfitting
    1. aliyun: submission_notebook.csv.04-07-2018_22-19-00, LB:0.9538
44, ensemble of best FFM(submission_notebook.csv.04-07-2018_22-19-00) and
    best LGM(submission_notebook.csv.04-04-2018_15-08-53):
    1. aliyun: LB: 0.9686
45, FFM of full training data, qcut 100, same qcut bins with training for val and test,
    remove ip count to reduce overfitting, ffm iter:40, r 0.078
    1. aliyun: LB:0.9614 submission_notebook.csv.04-08-2018_08-38-26
46, FFM of full training data, qcut 300, same qcut bins with training for val and test,
    remove ip count to reduce overfitting, ffm iter 40, r 0.078:
    1. aliyun: LB: 0.9395
47, ensemble 46 + 38
    1. aliyun: LB: 0.9691
48, 46 + downsample training negative labels to 1/5
    1. aliyun: LB: 0.9557
49, 48 + r 0.15:
    1. aliyun: 0.9404
49, 48 + r 0.01:
    1. aliyun: 0.9271
50, use train/val split and 1/6 sample for ffm
    1. aliyun: auc: 0.9637972262918038
51, 38 + 1/6 train, 1/6 val, new lgbm parameters
    1. grid test: train's auc: 0.986087	valid's auc: 0.969878
52, 51 + scale_pos_weight 200
    1. new val test: train's auc: 0.986333	valid's auc: 0.969725
53, 52 + new features: ip_day_hhcount:
    2. grid test 2:train's auc: 0.988149	valid's auc: 0.968851
54, 51 + new features: ip_day_hhcount:
    2. grid test 2: train's auc: 0.985928	valid's auc: 0.969962
55, 51 + new features: ip_day_hhcount + remove some fts:
    1. grid test : train's auc: 0.987025	valid's auc: 0.971098
56, 52 + scale_pos_weight 200
    1. grid test: train's auc: 0.985561	valid's auc: 0.970922
57, 52 + FFM hyper parameter grid search:
    1. r 0.078, t 40: 0.9638428489959663
    2. r 0.15, t40: 0.9640482258163178
    3, r 0.078, t 30: 0.9635215841365019
    4. r 0.11, t 40: 0.9640808501138804
    5. r 0.11, t 30: 0.9638179245603409
    6. r 0.11, t 100: 0.9638509291435216
    7. r 0.11, t 70:0.9639892505480051
58, use new fts to train FFM, r 0.11, t 40:
    1. aliyun: 0.966384372004959
59, 58 + discretization 50:
    1. aliyun: 0.9677942031706754
60. 55 + add app, os, device, channel,hour count features
    1. grid tests: train's auc: 0.985089	valid's auc: 0.97119
61. 58 + discretization 25/75:
    1. 25: aliyun: 0.9633614268965304
    2. 75: aliyun: 0.9666321543464464
62. 60 with no train/val sampling + predict
    grid test 2: train's auc: 0.973703	valid's auc: 0.975377
    submission_notebook.csv.04-09-2018_01-27-57 LB: 0.9696
63. 58 + discr 75 + new counts features in 60
    aliyun: 0.9607340860846558
64. 58 + discr 50 + new counts features in 60
    aliyun: 0.9607340860846558
65. 55 + no sampling and predict:
    grid test: train's auc: 0.977749	valid's auc: 0.978409
    submission_notebook.csv.csv.04-09-2018_02-36-33 LB: 0.9702
66. 55+2 hist statis features:
    grid test 2: failed with mem
    new val test: testing per day version: train's auc: 0.984178	valid's auc: 0.97074
67. 59 + no sample and predict
    aliyun: 0.976217, predict: LB: 0.9688
    submission_notebook.csv.04-09-2018_11-50-46
68. 65 + filter app/channel:
    1. filter app 12: grid test runing:train's auc: 0.816762	valid's auc: 0.799485
        submission_notebook.csv.04-09-2018_12-03-29
    2. filter channel: grid test 2: train's auc: 0.870514	valid's auc: 0.796264
        submission_notebook.csv.04-09-2018_12-05-15
    3. filter app 18 14: grid test 2: train's auc: 0.862989	valid's auc: 0.77133
        submission_notebook.csv.09-04-2018_04-18-14
    4. filter app 8 11: grid test: train's auc: 0.816762	valid's auc: 0.799485
        submission_notebook.csv.09-04-2018_04-14-21
69. ensemble of best so far FFM 67 and LGBM 65:
    ensemble_submission.csv.09-04-2018_12-24-34 LB:0.9705
70. ensemble of 69 + 68:
    1. aliyun: 0.9690
71. 66 + filter to only train and predict for data with hist features:
    1. aliyun: train's auc: 0.981938	valid's auc: 0.972096
    2. aliyun: train's auc: 0.982728	valid's auc: 0.97217
        submission_notebook.csv.10-04-2018_13-31-47
72. 68 + hyper params tunning for channel filter:
    1. old params no new fts: grid test: train's auc: 0.835371	valid's auc: 0.797188
    2. new params no new fts: grid test2: 0.799837
        submission_notebook.csv.09-04-2018_09-00-10
73. 68 + new para, no new fts:
    1. filter app 12: grid test 2: train's auc: 0.859417	valid's auc: 0.751798
        submission_notebook.csv.09-04-2018_15-59-01
    2. filter app 18 14: grid test: train's auc: 0.853057	valid's auc: 0.776124
        submission_notebook.csv.09-04-2018_16-00-44
    3. filter app 8 11: val test: train's auc: 0.825384	valid's auc: 0.806867
        submission_notebook.csv.09-04-2018_15-56-33
74. ensemble of 69 + 35+39+68.1 + 72.2
75. ensemble of 69 + 35+39+68.1 + 72.2 + word batch FM FTRL
76. 69 + next click feature ensemble:
    grid test: 0.9731
77. half data to train, andd next click feature: LB: 0.9724
    mac: train's auc: 0.995599	valid's auc: 0.986938
        submission_notebook.csv.10-04-2018_13-06-17
78. 77 + add click count later feature:
    1/6 grid test2: running
79.76 + historical feature(only hist data trained) ensemble
    aliyun:
80. 77 + ffm data gen
    ensemble_submission.csv.10-04-2018_18-49-00 LB:0.9715
81. add historical cvr features:
    aliyun:

to do test:
1. kaggle public kernel, most least frequent hours feature
2. break down auc by dimensions and filter data and train separately and ensemble
3. bag features
4. gbdt + ffm
5. gbdt feature + ffm
6. normalization
7. count smoothing: xxxx
8. unbalance labels
9. drop ip count feature, who's very overfitting according to ffm work
10. looking at some channel only, performs badly, consider remove channel+ip count feature that over-fits
11. cvr confidence smooth features
12. feature engineering