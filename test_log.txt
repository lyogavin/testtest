arc testing:

base @ 13:
	1/6 train, val sample, use all history feature, label 4:00-15:00, with casting uint16,
	qcut 0.98,scale_pos_weight:99.7

base @20:
	1/6 train, val sample, use all history feature, label 4:00-15:00, with casting uint16,
	qcut 0.98,scale_pos_weight:99.0

1. feature and label use only range from 4:00 - 15:00, with casting to uint16:
    1. train's auc: 0.982385	valid's auc: 0.966781
2. use all history feature, label 4:00-15:00, with casting to uint16:
    1. train's auc: 0.981373	valid's auc: 0.968077
    2. mac:train's auc: 0.981373	valid's auc: 0.968077
3. use all history feature, label 4:00-15:00, without casting to uint16:
    1. train's auc: 0.981373	valid's auc: 0.968077
4. use all history feature, label 4:00-15:00, without casting to uint16, qcut 0.95:
    1. kernel:train's auc: 0.982129	valid's auc: 0.968454
5. use all history feature, label 4:00-15:00, without casting to uint16, qcut 0.98:
    1. kernel:train's auc: 0.983909	valid's auc: 0.96873
6. use all history feature, label 4:00-15:00, without casting to uint16, qcut 0.95, discretization 500:
    1. mac: train's auc: 0.983119	valid's auc: 0.968293
7. use all history feature, label 4:00-15:00, without casting to uint16, qcut 0.95, scale_pos_weight:99.7:
    1. grid test kernel: train's auc: 0.983002	valid's auc: 0.968661, to merge to base
8. use all history feature, label 4:00-15:00, without casting to uint16, qcut 0.98, add channel, app  os count feature:
    1. grid test 2 kernel: train's auc: 0.982419	valid's auc: 0.968044
9. use all history feature, label 4:00-15:00, with casting to uint16, qcut 0.99:
    1. new val kernel: train's auc: 0.978534	valid's auc: 0.967561
10. use all history feature, label 4:00-15:00, with casting to uint16, qcut 0.98:
    1. new val kernel: train's auc: 0.978534	valid's auc: 0.967561
11. use all history feature, label 4:00-15:00, without casting to uint16, qcut 0.98, add channel count feature:
    1. new val split: train's auc: 0.980404	valid's auc: 0.968002
12. use all history feature, label 4:00-15:00, without casting to uint16, qcut 0.98, add  app count feature:
    1. grid test 2: train's auc: 0.982621	valid's auc: 0.967913
13. use all history feature, label 4:00-15:00, with casting to uint16, qcut 0.98, scale_pos_weight:99.7
    1. new val split: train's auc: 0.982792	valid's auc: 0.968499
14. use all history feature, label 4:00-15:00, with casting to uint16, qcut 0.98, scale_pos_weight:99.3
    1. grid test 2: train's auc: 0.982592	valid's auc: 0.968233
15. use all history feature, label 4:00-15:00, with casting to uint16, qcut 0.98, scale_pos_weight:98.5
    1. new val split: train's auc: 0.983195	valid's auc: 0.968301
16. use all history feature, label 4:00-15:00, with casting to uint16, qcut 0.98, scale_pos_weight:99.9
    1. grid test: train's auc: 0.981288	valid's auc: 0.968207
17. use all history feature, label 4:00-15:00, with casting to uint16, qcut 0.98, scale_pos_weight:98.8
    1. grid test: train's auc: 0.983375	valid's auc: 0.96822
18. use all history feature, label 4:00-15:00, with casting to uint16, qcut 0.98, scale_pos_weight:98.9
    1. grid test 2: train's auc: 0.980357	valid's auc: 0.96775
19. use all history feature, label 4:00-15:00, with casting to uint16, qcut 0.98, scale_pos_weight:99.1
    1. new val split: train's auc: 0.981529	valid's auc: 0.968157
20. use all history feature, label 4:00-15:00, with casting to uint16, qcut 0.98, scale_pos_weight:99.0
    1. grid test: train's auc: 0.983909	valid's auc: 0.96873
    2. merge to base
21. use all history feature, label 4:00-15:00, without casting to uint16, qcut 0.98, scale_pos_weight:99.7
    1. new val split: train's auc: 0.981529	valid's auc: 0.968157
22. use all history feature, label 4:00-15:00, with casting to uint16, qcut 0.95, scale_pos_weight:99.7
    1. grid test: train's auc: 0.983002	valid's auc: 0.968661
23. base + add frequent/infrequent test hours features:
    1. grid test: train's auc: 0.983929	valid's auc: 0.968495
24. base + only the current day's data to extract features
    1. grid test: train's auc: 0.983696	valid's auc: 0.969374
25. base + only the current day's data to extract features + 2 times more data(1/3)
    1. grid test2: train's auc: 0.98291	valid's auc: 0.97013
26. base + only the current day's data to extract features + all training data
    1. new val split: train's auc: 0.98287	valid's auc: 0.972121
    2. new val split: train's auc: 0.98287	valid's auc: 0.972121
27. base + only the current day's data to extract features + all training data + use 09 to train
    1. aliyun:  train's auc: 0.97724    valid's auc: 0.97743     LB: 0.9693,
        submission_notebook.csv.04-04-2018_03-33-26@aliyun
28. base + remove last two features
    1. aliyun: train's auc: 0.982993	valid's auc: 0.968284
29. base + only the current day's data to extract features + all training data + hh feature
    1. grid test2: train's auc: 0.982993	valid's auc: 0.970014
30. base + only the current day's data to extract features 1/6 + hh feature
    1. grid test: 0.983704	valid's auc: 0.969496
31. base + filter 3 channels
    1. grid test: train's auc: 0.853195	valid's auc: 0.799714
    2. grid test: submission_notebook.csv.04-04-2018_03-34-54
32. base + filter 3 channels + hh feature + predict
    1. grid test: train's auc: 0.831272	valid's auc: 0.795663,
        submission_notebook.csv.04-04-2018_01-35-01
33. base + filter 2 apps(3,9) + predict
    1. grid test 2: train's auc: 0.867201	valid's auc: 0.86065
    2. grid test 2: predict
        submission_notebook.csv.04-04-2018_04-41-45
34. base + filter 1 apps(12) + predict
    1. new val split: train's auc: 0.807762	valid's auc: 0.752095
        submission_notebook.csv.04-04-2018_13-30-39
35. base + filter 2 apps(18 14) + predict
    1. grid test 2: train's auc: 0.846225	valid's auc: 0.778439
        submission_notebook.csv.04-04-2018_11-12-10
36. ensemble of 27+31+33: LB 0.9694
37. base + filter 2 apps(8 11) + predict
    1. grid test : train's auc: 0.770682	valid's auc: 0.605115
        submission_notebook.csv.04-04-2018_12-58-05
        ^^^^^^^^^^^^  there's a bug to this versoin deleted
38. base + different lgbm hyperparameters: # num_leaves : 7  ->  9, max_depth  :  4  ->  5, subsample  : 0.7 -> 0.9
    1. grid test 2: train's auc: 0.978664	valid's auc: 0.97764 LB:0.9696
        submission_notebook.csv.04-04-2018_15-08-53
39. base + filter 2 apps(8 11) + predict + different lgbm hyperparameters:
        # num_leaves : 7  ->  9, max_depth  :  4  ->  5, subsample  : 0.7 -> 0.9
    1. grid test :train's auc: 0.832043	valid's auc: 0.818861
        submission_notebook.csv.04-04-2018_16-23-14
40. base + filter 1 apps(12) + predict + different lgbm hyperparameters:
        # num_leaves : 7  ->  9, max_depth  :  4  ->  5, subsample  : 0.7 -> 0.9
    1. grid test 2: train's auc: 0.866256	valid's auc: 0.757562
        submission_notebook.csv.04-04-2018_16-27-49
41. ensemble of models:
    a, submission_notebook.csv.04-04-2018_03-33-26, 27, all training data, params old
    b, submission_notebook.csv.04-04-2018_03-34-54, 31, 3 channels
    c, submission_notebook.csv.04-04-2018_04-41-45, 33, filter 2 apps:3 9
    d, submission_notebook.csv.04-04-2018_11-12-10, 35, filter 2 apps:18 14
    e, submission_notebook.csv.04-04-2018_13-30-39, 34, filter 1 app:12
    f, submission_notebook.csv.04-04-2018_15-08-53, 38, all training data, new params
    LB: 0.9697
42, FFM of full training data:
    1. aliyun: very bad, because of too big cardinality, need to qcut to 1000
43, FFM of full training data, qcut 100, same qcut bins with training for val and test,
    remove ip count to reduce overfitting
    1. aliyun: submission_notebook.csv.04-07-2018_22-19-00, LB:0.9538
44, ensemble of best FFM(submission_notebook.csv.04-07-2018_22-19-00) and
    best LGM(submission_notebook.csv.04-04-2018_15-08-53):
    1. aliyun: LB: 0.9686
45, FFM of full training data, qcut 100, same qcut bins with training for val and test,
    remove ip count to reduce overfitting, ffm iter:40, r 0.078
    1. aliyun: LB:0.9614 submission_notebook.csv.04-08-2018_08-38-26
46, FFM of full training data, qcut 300, same qcut bins with training for val and test,
    remove ip count to reduce overfitting, ffm iter 40, r 0.078:
    1. aliyun: LB: 0.9395
47, ensemble 46 + 38
    1. aliyun: LB: 0.9691
48, 46 + downsample training negative labels to 1/5
    1. aliyun: LB: 0.9557
49, 48 + r 0.15:
    1. aliyun: 0.9404
49, 48 + r 0.01:
    1. aliyun: 0.9271
50, use train/val split and 1/6 sample for ffm
    1. aliyun: auc: 0.9637972262918038
51, 38 + 1/6 train, 1/6 val, new lgbm parameters
    1. grid test: train's auc: 0.986087	valid's auc: 0.969878
52, 51 + scale_pos_weight 200
    1. new val test: train's auc: 0.986333	valid's auc: 0.969725
53, 52 + new features: ip_day_hhcount:
    2. grid test 2:train's auc: 0.988149	valid's auc: 0.968851
54, 51 + new features: ip_day_hhcount:
    2. grid test 2: train's auc: 0.985928	valid's auc: 0.969962
55, 51 + new features: ip_day_hhcount + remove some fts:
    1. grid test : train's auc: 0.987025	valid's auc: 0.971098
56, 52 + scale_pos_weight 200
    1. grid test: train's auc: 0.985561	valid's auc: 0.970922
57, 52 + FFM hyper parameter grid search:
    1. r 0.078, t 40: 0.9638428489959663
    2. r 0.15, t40: 0.9640482258163178
    3, r 0.078, t 30: 0.9635215841365019
    4. r 0.11, t 40: 0.9640808501138804
    5. r 0.11, t 30: 0.9638179245603409
    6. r 0.11, t 100: 0.9638509291435216
    7. r 0.11, t 70:0.9639892505480051
58, use new fts to train FFM, r 0.11, t 40:
    1. aliyun: 0.966384372004959
59, 58 + discretization 50:
    1. aliyun: 0.9677942031706754
60. 55 + add app, os, device, channel,hour count features
    1. grid tests: train's auc: 0.985089	valid's auc: 0.97119
61. 58 + discretization 25/75:
    1. 25: aliyun: 0.9633614268965304
    2. 75: aliyun: 0.9666321543464464
62. 60 with no train/val sampling + predict
    grid test 2: train's auc: 0.973703	valid's auc: 0.975377
    submission_notebook.csv.04-09-2018_01-27-57 LB: 0.9696
63. 58 + discr 75 + new counts features in 60
    aliyun: 0.9607340860846558
64. 58 + discr 50 + new counts features in 60
    aliyun: 0.9607340860846558
65. 55 + no sampling and predict:
    grid test: train's auc: 0.977749	valid's auc: 0.978409
    submission_notebook.csv.csv.04-09-2018_02-36-33 LB: 0.9702
66. 55+2 hist statis features:
    grid test 2: failed with mem
    new val test: testing per day version: train's auc: 0.984178	valid's auc: 0.97074
67. FFM 59 + no sample and predict
    aliyun: 0.976217, predict: LB: 0.9688
    submission_notebook.csv.04-09-2018_11-50-46
68. 65 + filter app/channel:
    1. filter app 12: grid test runing:train's auc: 0.816762	valid's auc: 0.799485
        submission_notebook.csv.04-09-2018_12-03-29
    2. filter channel: grid test 2: train's auc: 0.870514	valid's auc: 0.796264
        submission_notebook.csv.04-09-2018_12-05-15
    3. filter app 18 14: grid test 2: train's auc: 0.862989	valid's auc: 0.77133
        submission_notebook.csv.09-04-2018_04-18-14
    4. filter app 8 11: grid test: train's auc: 0.816762	valid's auc: 0.799485
        submission_notebook.csv.09-04-2018_04-14-21
69. ensemble of best so far FFM 67 and LGBM 65:
    ensemble_submission.csv.09-04-2018_12-24-34 LB:0.9705
70. ensemble of 69 + 68:
    1. aliyun: 0.9690
71. 66 + filter to only train and predict for data with hist features:
    1. aliyun: train's auc: 0.981938	valid's auc: 0.972096
    2. aliyun: train's auc: 0.982728	valid's auc: 0.97217
        submission_notebook.csv.10-04-2018_13-31-47
72. 68 + hyper params tunning for channel filter:
    1. old params no new fts: grid test: train's auc: 0.835371	valid's auc: 0.797188
    2. new params no new fts: grid test2: 0.799837
        submission_notebook.csv.09-04-2018_09-00-10
73. 68 + new para, no new fts:
    1. filter app 12: grid test 2: train's auc: 0.859417	valid's auc: 0.751798
        submission_notebook.csv.09-04-2018_15-59-01
    2. filter app 18 14: grid test: train's auc: 0.853057	valid's auc: 0.776124
        submission_notebook.csv.09-04-2018_16-00-44
    3. filter app 8 11: val test: train's auc: 0.825384	valid's auc: 0.806867
        submission_notebook.csv.09-04-2018_15-56-33
74. ensemble of 69 + 35+39+68.1 + 72.2
75. ensemble of 69 + 35+39+68.1 + 72.2 + word batch FM FTRL
76. 69 + next click feature ensemble:
    grid test: 0.9731
77. half data to train, andd next click feature: LB: 0.9724
    mac: train's auc: 0.995599	valid's auc: 0.986938
        submission_notebook.csv.10-04-2018_13-06-17
78. 77 + add click count later feature:
    1/6 grid test2:
79.76 + historical feature(only hist data trained) ensemble
    aliyun:
80. 77 + ffm data gen
    ensemble_submission.csv.10-04-2018_18-49-00 LB:0.9715
81. add historical cvr features:
    mac: old params: bug
    aliyun: new params: train's auc: 0.985981   valid's auc: 0.974437
    Feature names: ['app', 'device', 'os', 'channel', 'hour', 'ip_day_hourcount', 'ip_day_hour_oscount', 'ip_day_hour_appcount', 'ip_day_hour_app_oscount', 'app_day_hourcount', 'ip_in_t
est_hhcount', 'next_click', 'ip_device_cvr', 'ip_app_device_os_channel_cvr', 'app_channel_cvr', 'ip_cvr', 'app_cvr', 'device_cvr', 'os_cvr', 'channel_cvr', 'app_os_cvr', 'app_device
_cvr']
    Feature importances: [194, 27, 151, 280, 32, 71, 18, 16, 39, 30, 89, 120, 8, 2, 1, 6, 1, 2, 11, 7, 5, 2]
    aliyun: comparing without crv features: train's auc: 0.987447	valid's auc: 0.974464
    grid test:
82. according to feature importance, only keep: ip_device_cvr, ip_cvr, os_cvr, channel_cvr
    adding hour
    Feature names: ['app', 'device', 'os', 'channel', 'hour', 'ip_day_hourcount', 'ip_day_hour_oscount', 'ip_day_hour_appcount', 'ip_day_hour_app_oscount', 'app_day_hourcount', 'ip_in_test_hhcount', 'next_click', 'ip_device_hour_cvr', 'ip_hour_cvr', 'os_hour_cvr', 'channel_hour_cvr']
    Feature importances: [233, 28, 180, 356, 53, 96, 27, 32, 60, 30, 111, 135, 9, 2, 23, 17]
    train's auc: 0.987398	valid's auc: 0.974247
83. ip_device_cvr, ip_cvr, os_cvr, channel_cvr: click count when no attribution
    Feature names: ['app', 'device', 'os', 'channel', 'hour', 'ip_day_hourcount', 'ip_day_hour_oscount', 'ip_day_hour_appcount', 'ip_day_hour_app_oscount', 'app_day_hourcount', 'ip_in_test_hhcount', 'next_click', 'ip_device_cvr', 'ip_cvr', 'os_cvr', 'channel_cvr']
    Feature importances: [232, 28, 186, 361, 55, 98, 27, 29, 68, 36, 115, 140, 4, 15, 4, 2]
    train's auc: 0.987176	valid's auc: 0.974488
84. ip_device_os_app_channel_cvr: click count when no attribution
    aliyun::train's auc: 0.986444   valid's auc: 0.974269
    Feature names: ['app', 'device', 'os', 'channel', 'hour', 'ip_day_hourcount', 'ip_day_hour_oscount', 'ip_day_hour_appcount', 'ip_day_hour_app_oscount', 'app_day_hourcount', 'ip_in_test_hhcount', 'next_click', 'ip_app_device_os_channel_cvr']
    Feature importances: [214, 29, 153, 319, 44, 74, 13, 23, 45, 25, 95, 133, 17]
85. use ip_device_cvr, ip_cvr, os_cvr, channel_cvr: click count when no attribution features
    1/2 to train, 1/6 to val,
    aliyun:
        [166]	train's auc: 0.983734	valid's auc: 0.977309
        Feature names: ['app', 'device', 'os', 'channel', 'hour', 'ip_day_hourcount', 'ip_day_hour_oscount', 'ip_day_hour_appcount', 'ip_day_hour_app_oscount', 'app_day_hourcount', 'ip_in_test_hhcount', 'next_click', 'ip_device_cvr', 'ip_cvr', 'os_cvr', 'channel_cvr']
        Feature importances: [209, 37, 167, 363, 61, 66, 35, 31, 58, 42, 85, 148, 13, 9, 4, 0]
86. word_batch model
    1/6 train, 1/6 val,
    Grid test: val auc: 0.9639021425045405
87. 85 + calculate cvr with all days data,
    cvr + non attribution count features, 1/6 train
    1. aliyun part of features: train's auc: 0.987661	valid's auc: 0.974383
    Feature names: ['app', 'device', 'os', 'channel', 'hour', 'ip_day_hourcount', 'ip_day_hour_oscount', 'ip_day_hour_appcount', 'ip_day_hour_app_oscount', 'app_day_hourcount', 'ip_in_test_hhcount', 'next_click', 'ip_app_device_os_channel_non_attr_count', 'app_os_non_attr_count', 'app_channel_non_attr_count', 'app_device_non_attr_count', 'ip_app_device_os_channel_cvr', 'app_os_cvr', 'app_channel_cvr', 'app_device_cvr']
    Feature importances: [219, 30, 182, 396, 52, 88, 22, 29, 53, 31, 114, 130, 17, 4, 3, 0, 19, 21, 6, 8]
    2. all counting features:
    aliyun: train's auc: 0.986582	valid's auc: 0.974614
        Feature names: ['app', 'device', 'os', 'channel', 'hour', 'ip_day_hourcount', 'ip_day_hour_oscount', 'ip_day_hour_appcount', 'ip_day_hour_app_oscount', 'app_day_hourcount', 'ip_in_test_hhcount', 'next_click', 'ip_app_device_os_channel_non_attr_count', 'app_os_non_attr_count', 'app_channel_non_attr_count', 'app_device_non_attr_count', 'ip_device_non_attr_count', 'ip_non_attr_count', 'os_non_attr_count', 'channel_non_attr_count', 'ip_app_device_os_channel_cvr', 'app_os_cvr', 'app_channel_cvr', 'app_device_cvr', 'ip_device_cvr', 'ip_cvr', 'os_cvr', 'channel_cvr']
        Feature importances: [201, 30, 161, 293, 38, 75, 16, 19, 49, 24, 99, 118, 5, 1, 1, 0, 5, 2, 2, 0, 12, 4, 6, 6, 2, 2, 1, 4]
    fix issue:
        train's auc: 0.9867	valid's auc: 0.974757
        Feature names: ['app', 'device', 'os', 'channel', 'hour', 'ip_day_hourcount', 'ip_day_hour_oscount', 'ip_day_hour_appcount', 'ip_day_hour_app_oscount', 'app_day_hourcount', 'ip_in_test_hhcount', 'next_click', 'ip_app_device_os_channel_non_attr_count', 'app_os_non_attr_count', 'app_channel_non_attr_count', 'app_device_non_attr_count', 'ip_device_non_attr_count', 'ip_non_attr_count', 'os_non_attr_count', 'channel_non_attr_count', 'ip_app_device_os_channel_cvr', 'app_os_cvr', 'app_channel_cvr', 'app_device_cvr', 'ip_device_cvr', 'ip_cvr', 'os_cvr', 'channel_cvr']
        Feature importances: [224, 27, 182, 316, 38, 76, 15, 21, 56, 25, 112, 127, 6, 3, 3, 0, 1, 4, 1, 0, 0, 6, 1, 2, 5, 3, 5, 5]
    3. 87.2 -> full day train 1/6 val and predict:
        aliyun: train's auc: 0.983686	valid's auc: 0.97806
        submission_notebook.csv.14-04-2018_12-11-18
        LB: 0.9705
    4. fix sta ft issue: aliyun: train's auc: 0.983237	valid's auc: 0.978012
    Feature names: ['app', 'device', 'os', 'channel', 'hour', 'ip_day_hourcount', 'ip_day_hour_oscount', 'ip_day_hour_appcount', 'ip_day_hour_app_oscount', 'app_day_hourcount', 'ip_in_test_hhcount', 'next_click', 'ip_app_device_os_channel_non_attr_count', 'app_os_non_attr_count', 'app_channel_non_attr_count', 'app_device_non_attr_count', 'ip_device_non_attr_count', 'ip_non_attr_count', 'os_non_attr_count', 'channel_non_attr_count', 'ip_app_device_os_channel_cvr', 'app_os_cvr', 'app_channel_cvr', 'app_device_cvr', 'ip_device_cvr', 'ip_cvr', 'os_cvr', 'channel_cvr']
    Feature importances: [183, 31, 157, 301, 38, 51, 28, 13, 51, 49, 77, 131, 10, 1, 4, 0, 4, 7, 1, 0, 2, 5, 5, 4, 8, 4, 5, 6]
        submission_notebook.csv.16-04-2018_02-03-52 LB: 0.9704
    5. keep only     ['ip', 'app', 'device', 'os', 'channel'],['ip','device'],['ip']
        aliyun: train's auc: 0.985055   valid's auc: 0.977926
        submission_notebook.csv.16-04-2018_09-33-48
        LB: 0.9704
    6. new new params, ft: add day,
        grid test 4: train's auc: 0.985255	valid's auc: 0.973533
        grid test 4: full day:




88. 86 + count feature discretization 50:
    1. grid test:0.9655061249593924
    2. use full day to train and predict:
        grid test: AUC: 0.9725078983872897
        0.9724989484846307
        0.9727059746066206
    3. log discretization full with na/qcut fix
        grid test: 0.9734831898601476
        wordbatch_fm_ftrl.csv.12-04-2018_17-34-12
        LB:0.9544
    4. log dis 1/6
        val test:0.9727262372348875
        fix na, fix qcut : val test: 0.9734974236780696

89. 86 + log discretization:
    1. grid test:0.964113908432143
    2. use feature acronyms as prefix:
        grid test: 0.966414
    3. add interactive features:
        grid test 2: 0.9672690870802338
    4. full to train with interactive
        grid test2: , 0.9733642574137581, LB: 0.9611
            wordbatch_fm_ftrl.csv.13-04-2018_09-05-32

        grid test2: another run after bug fix: LB: 0.9601
            89 with ip:0.972379470620777
            wordbatch_fm_ftrl.csv.16-04-2018_01-34-55 LB: 0.9596

        Aliyun: fixed bug: wordbatch_fm_ftrl.csv.14-04-2018_19-12-50
            LB: 0.9670

        remov ip:
            grid test3: 0.9719423646799616:wordbatch_fm_ftrl.csv.16-04-2018_01-30-30 LB: to run


    5. use all days data to train in streaming:
        grid test2:
    6. add ip feature:
        grid test: 0.967267704435634(0.9672534709722984)(0.9677147448307044)
    7. try public wordbatch kernel combination features:
        val test: 0.9648907667401769
    8. FTRL:
        grid test 4: train_config_89_8
            0.9709744819068332:wordbatch_fm_ftrl.csv.16-04-2018_01-30-19 LB: 0.9583
    9. DNN:
        grid test: 0.968752617784948
        wordbatch_fm_ftrl.csv.16-04-2018_02-04-54 LB: to run




90. 88.4 + NN H1:
    1 .grid test2: 0.93233640757455
    2. full predict: 0.9506551726083916
91. 88.4+FTRL:
    1. val test: 0.9589079351648598
    2. full predict: 0.9686807551924094

92. ensemble of public wordbatch + 80 + 77 + 87.3
    LB: 0.9741

93. LGBM train only:
    1. base:
    grid test3:
    2. grid test4:

94. LGBM train predict:
    1. base:
        grid test4: failed mem
        mac: train's auc: 0.986462	valid's auc: 0.974841
        grid test4: failed
    2. 1/2 to train and 1/2 to val: grid test 4:
    3. 1/6 train, 1/6 val:
        train's auc: 0.987653	valid's auc: 0.974953
    4. train/val/test seperatedly
        grid test 4:
            train's auc: 0.979488	valid's auc: 0.980079
            LB: 0.9734
            submission_notebook.csv.17-04-2018_11-05-44
    5. use params from search id 0_35
        grid test 4: train_config_94_5:
            [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves
            train's auc: 0.9827	valid's auc: 0.978851
            submission_notebook.csv.17-04-2018_16-51-25
            LB:0.9704
    6. use features from selection search, lgbm_params_from_search_0_35:
        new val split:
            use 1/2 to train: train's auc: 0.98148	valid's auc: 0.977288
                submission_notebook.csv.17-04-2018_17-47-09
                LB:0.9669
    7. use params from search id 0_81,
        grid test 4: train_config_94_5:
            train's auc: 0.98209	valid's auc: 0.976457
            submission_notebook.csv.17-04-2018_18-00-41
                LB: 0.9688
    8. use features from selection search, use lgbm new params, use 1/2 to train:
        new val split: train's auc: 0.979721	valid's auc: 0.979057
            submission_notebook.csv.18-04-2018_01-14-40
                LB: 0.9689
    9. use params from search id 0_81 + no no_estimators, use early stopping.
        grid test 4:
        submission_notebook.csv.18-04-2018_01-13-31
        LB:
    10. use features from selection search, remove least important features,
        use lgbm new params, use 1/2 to train:
        new val split: train's auc: 0.981166	valid's auc: 0.979353
            submission_notebook.csv.18-04-2018_02-43-00
                LB:
    11. 10 + old features to reproduce 1/2 day train:
        new val split: train's auc: 0.980093	valid's auc: 0.979113
            submission_notebook.csv.18-04-2018_03-46-54.csv
            LB:0.9724
    12. 11+ best search single fts group:
        new val split: train's auc: 0.980249	valid's auc: 0.97967
            submission_notebook.18-04-2018_06-40-13.csv
            LB: reached limit
    13. 1/6 with new lgbm params from 96.6 best
        grid test 4: train's auc: 0.975135	valid's auc: 0.974865
    14. 1/6 with lgbm params search from 96.8:
    'colsample_bytree': 0.6021467085338628, 'learning_rate': 0.37861782981986614, 'max_delta_step': 2, 'max_depth': 5, 'min_child_samples': 171, 'min_child_weight': 1, 'min_split_gain': 0.004337458691335552, 'n_estimators': 249, 'num_leaves': 10, 'reg_alpha': 8.1447463220916e-05, 'reg_lambda': 64.24440555484793, 'scale_pos_weight': 4.350080991126866, 'subsample': 0.5332391076871872, 'subsample_for_bin': 470584, 'subsample_freq': 1
        grid test : train's auc: 0.982759	valid's auc: 0.977399
        predict: with 1day:
        LB: 0.9726
    15. 1day, lgbm_params_from_search_2_11,
        auc: 9748
        {'colsample_bytree': 0.01, 'learning_rate': 1.040731554567982, 'max_depth': 0, 'min_child_samples': 51, 'min_child_weight': 7, 'min_split_gain': 1.0109536459124555, 'n_estimators': 300, 'num_leaves': 31, 'reg_alpha': 1.2420399086947274, 'reg_lambda': 1000.0, 'scale_pos_weight': 4.652061504081354, 'subsample': 1.0, 'subsample_for_bin': 571876, 'subsample_freq': 0}
        grid test: LB: 0.9682


95. 77 grid test 3 reproducing:
    0. full day to train and predict:
        grid test 3:
        train's auc: 0.980809	valid's auc: 0.979393
            submission_notebook.csv.16-04-2018_14-15-40
    1. use 1/6 to train to test
        train's auc: 0.986501	valid's auc: 0.974407
96. sklearn bayesSearchCV, params search:
    1. 93_1 config, n_jobs = 1, refit=False, iterations 30 grid test 2: done
    2. 93_1 config, n_jobs = 1, refit=False, iterations 100 grid test 2: version 76 done.
    3. 93_1 config, n_jobs = 1, refit=False, iterations 100 grid test 3: done
    4. 93_1 config, n_jobs = 1, refit=False, iterations 100 grid test: done
    5. 93_1 config, n_jobs = 1, refit=False, iterations 100 grid test 2: failed
    6. 93_1 config, n_jobs = 1, refit=False, iterations 20 changed to n_estimators: (50, 100, 300, 500)
        grid test 3: best:0.974774
        "{'colsample_bytree': 0.01, 'learning_rate': 1.040731554567982, 'max_depth': 0, 'min_child_samples': 51, 'min_child_weight': 7, 'min_split_gain': 1.0109536459124555, 'n_estimators': 300, 'num_leaves': 31, 'reg_alpha': 1.2420399086947274, 'reg_lambda': 1000.0, 'scale_pos_weight': 4.652061504081354, 'subsample': 1.0, 'subsample_for_bin': 571876, 'subsample_freq': 0}"
    7. 93_1 config, n_jobs = 1, refit=False, iterations 30 changed to n_estimators:n_estimators': (100, 200, 300, 400, 500)
        grid test2 : Failed
    8. 6 + refined search ranges:
        grid test: succeeded
        best auc: 0.977103,
        'colsample_bytree': 0.6021467085338628, 'learning_rate': 0.37861782981986614, 'max_delta_step': 2, 'max_depth': 5, 'min_child_samples': 171, 'min_child_weight': 1, 'min_split_gain': 0.004337458691335552, 'n_estimators': 249, 'num_leaves': 10, 'reg_alpha': 8.1447463220916e-05, 'reg_lambda': 64.24440555484793, 'scale_pos_weight': 4.350080991126866, 'subsample': 0.5332391076871872, 'subsample_for_bin': 470584, 'subsample_freq': 1
    9. 8. another run:
        grid 3: 0.975941
        {'colsample_bytree': 0.9991702362401564, 'learning_rate': 1.0, 'max_depth': 7, 'min_child_samples': 200, 'min_child_weight': 2, 'min_split_gain': 0.0, 'n_estimators': 131, 'num_leaves': 7, 'reg_alpha': 0.00014471896477199475, 'reg_lambda': 176.4050928547651, 'scale_pos_weight': 0.8349642426002523, 'subsample': 0.9658854015682086, 'subsample_for_bin': 679915, 'subsample_freq': 1}
    10. another run:
        grid test 3: best: 0.977468
        "{'colsample_bytree': 0.9832436408235254, 'learning_rate': 0.21148409343315033, 'max_depth': 3, 'min_child_samples': 116, 'min_child_weight': 1, 'min_split_gain': 0.0, 'n_estimators': 229, 'num_leaves': 6, 'reg_alpha': 4.597782653285172e-06, 'reg_lambda': 10.497593218316116, 'scale_pos_weight': 499.99999999999994, 'subsample': 0.38649766667697655, 'subsample_for_bin': 468639, 'subsample_freq': 0}"
    11. another run:
        val split: best: 0.976008
        {'colsample_bytree': 0.6209085649172932, 'learning_rate': 0.35540927532494104, 'max_depth': 6, 'min_child_samples': 176, 'min_child_weight': 3, 'min_split_gain': 0.005946200350868385, 'n_estimators': 342, 'num_leaves': 7, 'reg_alpha': 0.004026635957416632, 'reg_lambda': 0.040887904512512056, 'scale_pos_weight': 109.72255122430063, 'subsample': 0.6612742297240571, 'subsample_for_bin': 567047, 'subsample_freq': 0}
    12. another run:
        grid test 3: best: 0.975877

    13. another run:
        val split: best: 0.975669




97. grid search ft gen:
    aliyun: done:

98. grid search feature combinatoins:
    use config 98: aliyun :killed in half
99. 1/20 sample to train with all the combination features:
    use config 99
    1. aliyun: done. train's auc: 0.987152   valid's auc: 0.968834

    194                device_os_hour_is_attributedcount   3
    88                                      app_hourmean   3
    277                         app_channel_hourcumcount   3
    166                            app_channel_ipnunique   4
    150         app_device_os_hour_ip_is_attributedcount   4
    20                          os_ip_is_attributedcount   5
    278                  device_os_ip_is_attributedcount   5
    342                   app_channel_is_attributedcount   5
    103                app_os_hour_ip_is_attributedcount   7
    199                       hour_ip_is_attributedcount   9
    319                     device_ip_is_attributedcount  14
    6                   ip_in_test_hh_is_attributedcount  17
    110                            ip_is_attributedcount  27
    2                                                 os  38
    5    ip_app_device_os_channel_is_attributednextclick  48
    0                                                app  72
    3                                            channel  84

    2. another run, changed to 1/20, early stopping: 100:
    aliyun: train's auc: 0.990977	valid's auc: 0.969237
     ('app_channel_ipnunique', 3),
     ('device_os_hour_is_attributedcount', 3),
     ('app_device_ip_is_attributedcount', 3),
     ('device_channel_hour_ipcumcount', 3),
     ('app_ipvar', 3),
     ('channel_is_attributedcount', 3),
     ('device_channelcumcount', 4),
     ('device_os_hour_ipnunique', 4),
     ('app_os_channel_ip_is_attributedcount', 4),
     ('app_ip_is_attributedcount', 4),
     ('device', 5),
     ('device_hour_ip_is_attributedcount', 5),
     ('device_os_ip_is_attributedcount', 6),
     ('app_channel_is_attributedcount', 6),
     ('os_ip_is_attributedcount', 8),
     ('hour_ip_is_attributedcount', 9),
     ('app_os_hour_ip_is_attributedcount', 9),
     ('app_device_os_hour_ip_is_attributedcount', 11),
     ('ip_in_test_hh_is_attributedcount', 15),
     ('device_ip_is_attributedcount', 15),
     ('ip_is_attributedcount', 36),
     ('os', 53),
     ('ip_app_device_os_channel_is_attributednextclick', 68),
     ('app', 85),
     ('channel', 148)]
     3. skew -> next_click(and remove hard coded click)
        use base data cache = False
        use ft cache = False
        use config 99
        cache path -> ./ft_cache on kernel

        grid test3 : failed mem
     4. fts coms search + lgbm params search:
        skew -> next_click(and remove hard coded click)
        use base data cache = False
        use ft caceh = False
        use config
        cache path -> ./ft_cache on kernel

        grid test2: failed mem
     5. add another group of in_test_hh (and remove skew to speed up, and remvoe hard coded one)
        use base data cache = False
        use ft cache False
        use config 99
        cache path -> ./ft_cache on kernel


        val split : failed

100. use best auc from ft coms search:
    ft 0: [118]   train's auc: 0.984903   valid's auc: 0.974782
    Feature names: ['app', 'device', 'os', 'channel', 'hour',
    'app_device_ip_is_attributedcount', 'device_ipvar', 'device_os_ipmean', 'ip_is_attributedcount',
    'app_device_os_hour_ip_is_attributedcount', 'device_channel_is_attributedcount', 'next_click']
    Feature importances: [203, 25, 118, 247, 32, 28, 0, 4, 126, 27, 12, 122]
    1. grid : failed, changed to 1/2 sample to run
    2. gird test 4: use all to train to run again: LB: 0.9721
    3. grid test: 1 day train, use fts from search 99.1 0.9725
    4. grid test 4: use 1 day train, 99.2 LB: 0.9726
    5. grid test 4: based on 4 add importance 4 4 features half train: LB: 0.9728


101 lgbm params search:
    train_config_96, n_jobs = 3, refit=True, iterations 1000
    aliyun: killed
    Best ROC-AUC: 0.9779 so far

102 use new lgbm param:
    grid test:
    train's auc: 0.982307	valid's auc: 0.97766
    LB: 0.9702

    Best ROC-AUC: 0.9779
    {'min_child_weight': 3, 'subsample_for_bin': 542556, 'learning_rate': 0.30803266868575857, 'subsample_freq': 0, 'max_depth': 8, 'subsample': 1.0, 'num_leaves': 7, 'reg_lambda': 28.66709162037324, 'n_estimators': 371, 'min_split_gain': 0.0028642748250716932, 'reg_alpha': 7.351107378081451e-05, 'scale_pos_weight': 29.983824928443436, 'colsample_bytree': 0.8639270134191158, 'min_child_samples': 200}

103 train/val joint ft gen: compare with 94.4:
    use train_config_103
    1. aliyun: 1/2 day: LB:0.9724
    2. aliyun: 1 day train: LB:0.9737
    3. aliyun: use best lgbm search params (101): submission_notebook_20-04-2018_23-57-42.csv
    LB: 0.9736
    4. no qcut at all(qcut=0):
        aliyun: train's auc: 0.981078	valid's auc: 0.980541
        LB: 0.9738
    5. based on 4. add click_next shift
        LB: 0.9738
    6. 5 + pub fts:
        aliyun: LB: 0.9719
    7. 6 + pub fts, with 3 astype uint16:
        aliyun: 0.9728
    8. 7+pub ranges, no sample(still keep day ft)
        aliyun: LB: 0.9726
    9. 8 + dump features to debug:
        aliyun: done, still click to debug
    10. make everything the same, test (no_type_cast, type cast in config, no ft cache):
        aliyun: failed NAN
    11. remove channel from next click ft:
        based on 103_4, use ft cache, no day, no type cast = False
        train_config_103_11
        aliyun:submission_notebook_22-04-2018_23-32-09.csv(moved to /mnt)
            train's auc: 0.984493   valid's auc: 0.984067
            LB: 0.9791
    12. add mean, var, skew, min, qt98,qt02 of next click
        aliyun: stuck because of qt and skew too slow,
        aliyun: removed skew, qt rerun: train's auc: 0.982707	valid's auc: 0.983598
        LB: 0.9787
    13. 11 + use ft cache + keep only nc var:
        googleyun: train's auc: 0.983751   valid's auc: 0.984041
        LB: 0.9788
    14. 11 + next n click ft:
        googleyun: LB:0.9790
    15. try qcut:0.98
        googleyun: try1 killed OOM,
            moved nc to the head, LB:0.9788
    16. try removing app in next click dimensions:
        googleyun: train's auc: 0.985074	valid's auc: 0.984088
        LB:0.9789
        2: use mmh to retry: azzure:
    17. TODO:try add cvr of ip+app+os+device+hour/10min
    18. TODO: use next time range to fix last next click value too big(test supplyment and train next range)
    19. TODO: try smoothing of next click and next n click
        use average value to replace the too big value should be helpful
    20. add second feature
        googleyun: LB:0.9786
        2. +mmh3 try again: gyun:LB:0.9788
    21. try lgbm params from 115_0
        azzureyun: LB:0.9760
    22. add test data from test_supplyment to compute next_click:
        azzureyun: LB:0.9790, another run to try reproduce: LB: 0.9791
    23. try murmur hash : install mmh3
        gyun: LB:0.9791, another to check reproducibility: 0.9791
    24: 22 + mmh3
        azzureyun: LB: 0.9791
    25: based on 24 output some statistics of nc to make sure it's correctedly done
        gyun:
        with supplement: NAN next click count in test: 4556206
        without: done:
        NAN next click count in test: 5332803
    26. fix the time order confusion of val appended to train:
        use 09 3pm-09 4pm as val:
            azzureyun: LB:0.9781
    27. 26+ early stopping: 300
        grid test4: OOM
        azzureyun: LB:0.9791







104 seperate ft gen + try few lines of data following public kernel:
    use train_config_104
    compare with 94.4
    grid test3: LB:0.9718

105 use public lgbm params
    compare with 94.4
    grid test4: done version 38

106 test ft qcut:
    config 106
    1. baseline
        grid test 2: train's auc: 0.984589	valid's auc: 0.974076
    2. no qcut for next click (only qcut for count)
        grid test: train's auc: 0.987518	valid's auc: 0.974453
    3. joint ft gen:
        grid test2: train's auc: 0.988639	valid's auc: 0.974992
    4. 2+ joint ft gen:
        grid test: train's auc: 0.986187	valid's auc: 0.974787
    5. 3 + public lgbm params:
        grid test2: train's auc: 0.985341	valid's auc: 0.973539
    6. joint + no qcut all:
        grid test4: train's auc: 0.987148	valid's auc: 0.97487
        grid test4: early stopping 100: train's auc: 0.989302	valid's auc: 0.974815
    7. joint + noqcut + pub fts:
        val split: train's auc: 0.986388	valid's auc: 0.974555
        val split: early stopping 100: train's auc: 0.988386	valid's auc: 0.974489
    8. based on 3. add next click shift:
        grid test2: train's auc: 0.986457	valid's auc: 0.974897
    9. 8.+ pub ranges and day ft:
        grid test2: train's auc: 0.987384	valid's auc: 0.973594
    10. 3 + astypes.
        grid test3: train's auc: 0.986469	valid's auc: 0.974742
    11. 7+ astypes:
        grid test4: train's auc: 0.984605	valid's auc: 0.973873

107. ft coms grid search based on joint ft gen training:
    use config 99:
    aliyun: to run

108. use public fts train 1day seperately, no qcut
    1. old lgbm params: grid test: failed mem
    2. public lgbm params: grid test 3: failed mem
        grid test3: changed to 1_to_2 LB: 0.9709 again
    3. grid test3: use pub ranges and add day in categorical,
        train's auc: 0.982843	valid's auc: 0.967226
        LB: :0.9708
    4. based on 2. grid test: 1_to_2 to train, no cut, add click_next_shift, noday
        failed mem
    5.

109 pub script: aliyun: LB: 0.9777
110 pub script + remove astype uint16: azzureyun: LB: 0.9777
111 110 + use my train range: azzureyun: LB: 0.9786
112 111 + my lgbm params: azzureyun: LB: 0.9784
113 111 + add channel dimension in nextclick ft: azzureyun: 0.9724

114 use config 103_10 debug run to compare
    no need

115 lgbm param grid search: 08 val 09 train, sample 1/3
    azzureyun: Best ROC-AUC: 0.9826
    {'colsample_bytree': 1.0, 'learning_rate': 0.1773256374384233, 'max_depth': 3, 'min_child_samples': 200, 'min_child_weight': 0,
    'min_split_gain': 0.0007911719321269061, 'n_estimators': 500, 'num_leaves': 11, 'reg_alpha': 2.355979159306278e-08,
    'reg_lambda': 0.9016760858543618, 'scale_pos_weight': 260.6441151527916, 'subsample': 1.0, 'subsample_for_bin': 457694, 'subsample_freq': 0}
    1. try this best params in 103_21

116 streaming model
    1. 1/10 to test: auc: 0.9749064944799741
    2. full day to train:
        auc: 0.9755348263944024
        aliyun:LB: 0.9707 wordbatch_fm_ftrl.csv_23-04-2018_22-25-34.csv
    3. log_discretization true
        auc:0.9790121278867547
            wordbatch_fm_ftrl.csv_24-04-2018_01-12-29.csv
        aliyun: LB: 0.9728
    4. try discretization=50
        wordbatch_fm_ftrl.csv_24-04-2018_10-42-57.csv
        aliyun: LB: 0.9719
    6. DNN
        googleyun: killed OOM
        azzure cloud: testing renew wb every time(inherited ip, iter and D changes from 7.): LB: 0.9718
    5. FTRL
        aliyun: DONE LB: 0.9727
    7. following pub kernel's new updates:
        wb's D from 2** 22->2**20, FM_FTRL iter 3 -> 2, add ip feature
        gyun: 0.9729

117 FFM
    0. aliyun : LB: failed
    1. aliyun: fixed and LB:0.9750 (ffm_117_2_submission.csv moved to /mnt)
    2. TODO: remove shift feature and use 1/6 to rerun to test
    3. gen data seperately:
        azzure: LB:0.9776
    4. try pub fmftrl features:
        aliyun: LB:0.9762
    5. try use 083pm-093pm to train
        azzure: LB: 0.9775
    6. try log discretizatoin
        aliyun: 0.9780
    7. TODO: based on 5, do pick hours weighting
        currently libFFM doesn't support this.
    8. 3 + 6.
        aliyun: LB:0.9780
    9. add 20 LGBM features:
        aliyun: 0.9766
    10. 9+ 7 features to avoid overfitting and retry:
        Aliyun: LB:0.9775
    11. use only raw features to generate 20 lgbm features.
        A. gen model: grid test3: see 124_33
        B. use model generated from A
            aliyun: 0.9775
    12. new cvr features + interactive features + use 09 last 250w as val
        aliyun: 0.9767
        higher reg: nohup ./mark/mark1/mark1 -l 0.000005 -r 0.05 -s 12 -t 8 ./new_libffm_output/new_libffm_test.csv ./new_libffm_output/new_libffm_train.csv
        0.9777
    13. new libffm format
        azzureyun: 0.9322 <- bug: fix and rerun:
        LB: 0.9615
    14. following 117_8, refine the params to early stop to avoid overfitting
        azzureyun: running


118 ensemble of: 117_2, 116_3, 103_11
    LB: 0.9783
    2. 117_2 + 103_11:
        LB:0.9790

119 LGBM features:
    test with 1/20 first, aliyun: OK
    changed to 30 trees to gen 30 fts.
    aliyun: done
    2. aliyun: changed to 1/2 to run again. failed mem, 1/6 4 thread: OOM still OOM
    3. gyun run 1day done
120 online model with 30 LGBM features:
    aliyun: failed mem
    2. sync mode
    aliyun: failed mem
    3. changed to 1/2 to run,
    4. 1day gyun: LB: 0.9726
        wordbatch_fm_ftrl.csv_26-04-2018_08-47-35.csv
121 ft coms search:
    1. azzure: 1/3 with predict with predict failed OOM
       gyun: failed OOM
    2. 1/20 no predict , test next click instead of skew, no cumcount, aliyun: done in aliyun:nohup.log.0426
    3. 1/3, only keep count, nunique, next click: aliyun: OOM
    4. 1/3, only count and nextclick aliyun :OOM
    5. 1/3, only keep count, nunique, next click:
        gyun: OOM in training
    6. try different features set such as no count, only nunique, etc for better ensemble.
        1/3, only var and nunique, with nextclick and inhh:
        azzureyun: OOM in training
        aliyun: retry after fix lgbm mem, OOM
    7. 1/2 random sample, 08 train 09 val:
        nunique, 1 nextclick, 1 in hh
        gyun:
    [213]	train's auc: 0.980657	valid's auc: 0.968698
     ('hour_ipnunique', 8),
 ('os_channel_hour_ipnunique', 8),
 ('app_os_channelnunique', 9),
 ('app_device_channel_hour_ipnunique', 9),
 ('app_os_channel_ipnunique', 10),
 ('device_channel_hour_ipnunique', 10),
 ('app_os_ipnunique', 11),
 ('device_os_hour_ipnunique', 13),
 ('app_os_channel_hour_ipnunique', 13),
 ('app_hour_ipnunique', 14),
 ('device_os_ipnunique', 14),
 ('app_device_os_ipnunique', 14),
 ('app_os_hour_ipnunique', 14),
 ('app_device_os_channel_hour_ipnunique', 16),
 ('hour', 17),
 ('channel_ipnunique', 17),
 ('device_hour_ipnunique', 18),
 ('app_device_os_hour_ipnunique', 18),
 ('os_hour_ipnunique', 20),
 ('app_channel_hour_ipnunique', 24),
 ('app_channel_ipnunique', 34),
 ('device', 41),
 ('ip_app_device_os_is_attributednextclick', 134),
 ('ip_in_test_hh_is_attributedcount', 201),
 ('os', 267),
 ('app', 275),
 ('channel', 396)]
    8. based on 7, use count, 1/2 sample OOM, use 1/3 sample
        aliyun:
  [188]   train's auc: 0.9842     valid's auc: 0.971278
  ('device_os_is_attributedcount', 10),
 ('app_channel_hour_is_attributedcount', 10),
 ('device_os_ip_is_attributedcount', 10),
 ('app_device_os_channel_ip_is_attributedcount', 10),
 ('device_os_hour_ip_is_attributedcount', 11),
 ('app_is_attributedcount', 14),
 ('app_device_ip_is_attributedcount', 14),
 ('device_hour_ip_is_attributedcount', 14),
 ('app_device_channel_is_attributedcount', 15),
 ('app_device_os_ip_is_attributedcount', 16),
 ('app_channel_is_attributedcount', 20),
 ('app_os_ip_is_attributedcount', 20),
 ('app_device_os_hour_ip_is_attributedcount', 20),
 ('hour_ip_is_attributedcount', 21),
 ('app_os_hour_ip_is_attributedcount', 26),
 ('ip_in_test_hh_is_attributedcount', 31),
 ('device', 32),
 ('hour', 43),
 ('device_ip_is_attributedcount', 56),
 ('ip_app_device_os_is_attributednextclick', 60),
 ('ip_is_attributedcount', 101),
 ('os', 186),
 ('app', 230),
 ('channel', 347)]
    9. based on 7, try cvr with combined
        grid test3:
        azzureyun: OOM
        change to float16, 1/3 sample and retry: OOM

    10.based on 8, set L1 to 1.0
        aliyun:
    [135]	train's auc: 0.982748	valid's auc: 0.97085
  ('device_os_hour_is_attributedcount', 8),
 ('device_channel_hour_is_attributedcount', 8),
 ('app_os_channel_hour_ip_is_attributedcount', 8),
 ('app_device_channel_is_attributedcount', 9),
 ('app_device_os_ip_is_attributedcount', 9),
 ('app_os_hour_ip_is_attributedcount', 10),
 ('device_os_is_attributedcount', 12),
 ('device_hour_ip_is_attributedcount', 12),
 ('app_os_ip_is_attributedcount', 17),
 ('app_channel_is_attributedcount', 18),
 ('hour_ip_is_attributedcount', 18),
 ('device', 23),
 ('ip_in_test_hh_is_attributedcount', 25),
 ('app_device_os_hour_ip_is_attributedcount', 26),
 ('hour', 27),
 ('ip_app_device_os_is_attributednextclick', 32),
 ('device_ip_is_attributedcount', 45),
 ('ip_is_attributedcount', 85),
 ('os', 124),
 ('app', 178),
 ('channel', 249)]
    11. based on 7 L1 1.0:
        enable ad_val weighting to make it closer to test data set:
        azzureyun: failed didn't copy ad_val_model.txt
        TODO: fix and run later
    12. based on 8, with adversial enabled:
        grid test 4: OOM
        TODO: find other box to run
    13. smooth cvr fts:
        azzureyun:
        by split:
         ('app_os_is_attributedsmoothcvr', 2),
 ('ip_is_attributedsmoothcvr', 3),
 ('channel', 4),
 ('device_ip_is_attributedsmoothcvr', 4),
 ('os_hour_is_attributedsmoothcvr', 5),
 ('os', 11),
 ('app', 16),
 ('ip_in_test_hh_is_attributedcount', 16),
 ('hour_ip_is_attributedsmoothcvr', 18),
 ('channel_ip_is_attributedsmoothcvr', 20),
 ('app_ip_is_attributedsmoothcvr', 21),
 ('os_ip_is_attributedsmoothcvr', 27)]
        by gain:
         ('app_is_attributedsmoothcvr', 5071.2099609375),
 ('ip_app_device_os_is_attributednextclick', 6541.35986328125),
 ('channel', 33541.510009765625),
 ('device_ip_is_attributedsmoothcvr', 38582.2099609375),
 ('ip_is_attributedsmoothcvr', 49380.5791015625),
 ('app_os_is_attributedsmoothcvr', 56828.341796875),
 ('device', 58563.1015625),
 ('os_hour_is_attributedsmoothcvr', 70750.8408203125),
 ('os_channel_is_attributedsmoothcvr', 85324.796875),
 ('os', 148141.390625),
 ('ip_in_test_hh_is_attributedcount', 225224.80932617188),
 ('app', 424505.3664550781),
 ('hour_ip_is_attributedsmoothcvr', 1030227.6572265625),
 ('app_hour_is_attributedsmoothcvr', 1177230.0),
 ('os_ip_is_attributedsmoothcvr', 1650885.775390625),
 ('app_ip_is_attributedsmoothcvr', 16571897.869140625),
 ('channel_ip_is_attributedsmoothcvr', 62365126.38232422)]



122 onine gen ffm data:
    1. gyun: OOM
    2. TODO: try again
        gyun: OOM
123 pub fm_ftrl kernel 09769, seems like it's the training data size. (original 0.9769)
    try decrease to skip from 144708152 to test, forked kernel:
    batch size 1/2-> LB: 0.9718
    0. kernel: talkingdata-wordbatch-fm-ftrl-my-fts version 1. 0.9769
    1. forked kernel: changed to skip from 144708152 and 1/2 batch size: 0.9701
    2. azzureyun: changed only skip from 144708152, full batchsize to test perf: LB: 0.9701
    3. pub kernel: only 1/2 batch size,failed
        azzureyun: failed
    4. full batchsize, full data, normalization of features: pub kernel: LB: 0.9765/0.9767, worse
    5. skip from: 108735619, no norm, pub kernal, 1/2 batchsize: LB:0.9724
    6. use my best features:
        pub kernel: LB:0.9662
    7. full batchsize, skip from58735619: pub ker: OOM
    8. full batchsize, skip from 8 4am: azzureyun: OOM
    9. recover orginal train/val split, train FM-FTRL: use last 250w lines til 9 3pm as stacking val
        gyun: running
    10. DNN model: aliyun LB: 0.9710

124: LGBM seperately:
    0. train seperately with best params:
    aliyun: submission_notebook_27-04-2018_12-52-44.csv: LB:0.9792
    1. add 'day' dimension in all counting fts, and add day 8(4am-3pm) in training, use day 7 to test
        aliyun: LB: 0.9792
    2. 1+ add 7 also in training, use 6 in val
        gyun: LB:0.9791
    3. try train from midnight 08/3pm, val 08
        aliyun: LB:0.9793
        save model: azzureyun: Done
    [388]   train's auc: 0.984364   valid's auc: 0.983894
    4. TODO: normalization
        gyuN: TODO
    5. TODO: following 3, use more trainning data, see if we can play similar hour trick
        try train from 08/4am, to 9 4pm, 350 trees
        gyun: LB:0.9789
    6. more data: 5+ train from 7/3pm: 350 trees:
        gyun:LB: 0.9789
    7. try only try from night hours:
        grid test 4: OOM
    8. 3 + test hours weighted
        grid test 3: OOM
        gyun: copied to ~/, LB:0.9792
    [316]   train's auc: 0.984059   valid's auc: 0.983935
    9. 0 + pub FTRL features
        grid test 3: version 46: LB:0.9785
    10. based on 8, 600 iters, + change hour to 3 hours:
        gyun:  LB: 0.9785
    [600]   train's auc: 0.985895   valid's auc: 0.982256

    11. add best nunique fts from 121.7:
        grid test4: version 47: 0.9787
    12. TODO: based on 8, 600 iters,

    13. try all pub fts: TODO:
    14. 11+ feature_fraction 0.7
        grid test4: LB:0.9787
        [LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=0.7 will be ignored. Current value: feature_fraction=0.7.
    15. 11+ feature fraction 0.6
        grid test4:
        [451]	train's auc: 0.984614	valid's auc: 0.982889
    16. based on 8, try 415 iters, no val sample:
        gyun: LB: 0.9792
    [415]   train's auc: 0.984595   valid's auc: 0.98459
    17. train a model with 08 then merge.
        gyun: merging with 124_3:
        LB: 0.9795
        08's model only: LB:0.9785
    18(log incorrectly print 17). use adversial validation model to predict if it's test,
        use this score to weight training samples:
        aliyun: LB:0.9785
    19. 0+ try best lgbm params search set(3 too big to run in kernel, try 0 first):
    grid test 4: LB:0.9783 <-----bugged. rerun
    grid test 4: LB: 0.9783
    20. train a model with 7's data to merge with 17.
        azzureyun: LB:0.9795
        ^^^^^^^--------- was bugged, didn't deep copy config object.
        rerun: LB: 0.9795
    21. try use adversial validation scores as feature:
        aliyun: LB: 0.9792
        [('ad_val', 30),
         ('ip_day_hour_app_is_attributedcount', 49),
         ('ip_day_hour_app_os_is_attributedcount', 59),
         ('ip_day_hour_os_is_attributedcount', 61),
         ('device', 76),
         ('app_day_hour_is_attributedcount', 125),
         ('ip_day_hour_is_attributedcount', 146),
         ('ip_day_in_test_hh_is_attributedcount', 214),
         ('hour', 291),
         ('ip_app_device_os_day_is_attributednextclick', 353),
         ('os', 454),
         ('app', 506),
         ('channel', 772)]
    22. based on 18, normalize the weighting, to let min of weighing to be 1.0
        copy from 124 to let kernel able to run.
        A. grid test3: version 53: LB:0.9790
        only weight positive samples and rerun:
        B.     grid test3:LB:0.9784
    23. most frequent and leaset frequent channels in test data as feature:
        grid test2: LB: 0.9785
        [('ip_day_hour_app_os_is_attributedcount', 25),
         ('ip_day_hour_os_is_attributedcount', 29),
         ('ip_day_hour_app_is_attributedcount', 35),
         ('ip_in_test_hh_is_attributedcount', 53),
         ('device', 56),
         ('ip_day_hour_is_attributedcount', 56),
         ('hour', 58),
         ('app_day_hour_is_attributedcount', 65),
         ('ip_in_test_frequent_channel_is_attributedcount', 148),
         ('ip_app_device_os_is_attributednextclick', 244),
         ('os', 269),
         ('app', 309),
         ('channel', 461)]
         grid test 4: print gain importance and rerun:
         split:
         [('ip_day_hour_app_os_is_attributedcount', 25),
 ('ip_day_hour_os_is_attributedcount', 29),
 ('ip_day_hour_app_is_attributedcount', 35),
 ('ip_in_test_hh_is_attributedcount', 53),
 ('device', 56),
 ('ip_day_hour_is_attributedcount', 56),
 ('hour', 58),
 ('app_day_hour_is_attributedcount', 65),
 ('ip_in_test_frequent_channel_is_attributedcount', 148),
 ('ip_app_device_os_is_attributednextclick', 244),
 ('os', 269),
 ('app', 309),
 ('channel', 461)]
gain importance:
[('ip_day_hour_app_os_is_attributedcount', 304550.37890625),
 ('hour', 403344.6096191406),
 ('ip_day_hour_app_is_attributedcount', 403979.14611816406),
 ('device', 664619.6712036133),
 ('ip_day_hour_os_is_attributedcount', 664856.6567382812),
 ('ip_day_hour_is_attributedcount', 2017511.4193115234),
 ('app_day_hour_is_attributedcount', 4035449.3736572266),
 ('os', 4074379.297180176),
 ('ip_in_test_hh_is_attributedcount', 4910875.878601074),
 ('ip_in_test_frequent_channel_is_attributedcount', 5103478.940246582),
 ('ip_app_device_os_is_attributednextclick', 8369703.446411133),
 ('channel', 26717807.192504883),
 ('app', 101927083.53381348)]
    24. TODO: similar to 23, add most/least frequent other dimensions: app, os, device...
    25. 23+ad_val weighting
        grid test2: 0.9787
    26. train from best nunique and count from coms search:
        grid test4:
        [364]	train's auc: 0.984592	valid's auc: 0.982711
        LB:0.9775
    27. based on 23, change to 5% of least:
        TODO:
    28. based on 26, remove last 4 gain rerun:
    [237]	train's auc: 0.983657	valid's auc: 0.98331
        LB:0.9779 grid test4: version58
    29. add smoothcvr fts:
        azzureyun: 0.9789
    30. use 1 day before to calcuate smooth cvr:
        aliyun: 0.9789
    31. use new params from search:
    Best ROC-AUC: 0.9732

Best params: {'colsample_bytree': 0.7773614836495996, 'learning_rate': 0.2, 'max_depth': 10, 'min_child_samples': 10, 'min_child_weight': 0, 'num_leaves': 11, 'reg_alpha': 1.0, 'reg_lambda': 1e-09, 'scale_pos_weight': 249.99999999999994, 'subsample': 0.6870745956370757}
        grid test 4:LB: !!!!!!!!!!!!!!!BUGG!!!!!!, still runed 124_3
        grid test 2: run again: version 92: LB:0.9786
    32. based on 30, cvr features search:
        azzure: see 121_13
    33. only raw features to save model for FFM:
        grid test3: generated
    34. TODO: try only use dimensions combinations that showed up in test:
    35. hourly smooth cvr:
        grid test4: LB:0.9789
    36. smooth cvr from coms ft search 121_13:
        aliyun: LB:0.9788
    37. reduce features from 36
        azzureyun: LB: 0.9790
    38. following pub fm_ftrl's training time range:
        use 070am - 090am:
        gyun: 0.9788
    39. use 080am - 090am:
        aliyun: LB: 0.9790
    40. 124.3 + stacking val + reserve last 250w as stacking val:
        azzureyun: DONE
    41. based on 38, use 3 hours counting features to simulate streaming kernel:
        gyun: 0.9786 <-- should ensemble as multi-scale
    42. based on 124.3, use 15 min as hour to ensembel multi-scale
        azzureyun: LB: 0.9780
    43. similar to 41, 6 hours multiscale:
        based on 124.3
        aliyun: FAILED. TODO: to run later
    44. remove 'day' in counting fts and use more data:
        aliyun: TODO:
    45. no days fts, train with all data, train jointly, val last 250w, early_stopping:50
        gyun: LB: 0.9796812


125:TODO: how to incorperate the timeseries pattern from the pub kernel?
    1. add number of 10 minutes feature
    2. add average cvr of the current 10min
    3. ip day 10min count
    aliyun: LB:0.9774
    4. make it 30 min, change 30min to categorical feature and retry: aliyun: LB:0.9779
    5. TODO: make it 2hours and retry
        Grid test4:LB:0.9787
126. find best offline validation method:
    use 09-4am-3pm as val set
        08-4am-3pm as train, randomly sample 33%:
    1. 124 0. azzure:
    [283]   train's auc: 0.983571   valid's auc: 0.972195
    2. 124 2. gyun:killed
    3. 124 3. aliyun:killed
    4. 124 9. gyun:
    [148]	train's auc: 0.981603	valid's auc: 0.97152
    5. similar to 124 3, train: 073pm-083pm, val: 09
        azzure:
    [367]	train's auc: 0.983063	valid's auc: 0.972114
    6. similar to 124 8, train: 073pm-083pm, val: 09, with pick hours weighted
        aliyun:
    [415]	train's auc: 0.984266	valid's auc: 0.973415
    7. changed hour to 2 hours:
        grid test4:
    [297]	train's auc: 0.984024	valid's auc: 0.97134
    8. changed hour to 3 hours:
        grid test3:
    [305]	train's auc: 0.983763	valid's auc: 0.971294
    9. 126.1 + cat_smooth=100
        grid test3:
    [324]	train's auc: 0.984396	valid's auc: 0.971413
    10. change hour to 30 min:
        grid test2:
    [340]	train's auc: 0.984575	valid's auc: 0.971484
    11. feature_faction 0.5:
        grid test3:
    [383]	train's auc: 0.984053	valid's auc: 0.972068
    12. based on 6. try use 07 as val:
        grid test 3:
    [304]	train's auc: 0.982754	valid's auc: 0.977613
    13. use the asraful pub kernel's ft set:
        grid test 4:
    [307]	train's auc: 0.982766	valid's auc: 0.970275
    14. use pub asraful kernel's ft set and lgbm params:
        grid test 2:
    [302]	train's auc: 0.982995	valid's auc: 0.97019
    15. test smooth cvr features:
        azzureyun: DONE
    [201]	train's auc: 0.982792	valid's auc: 0.971219
    [230]   train's auc: 0.983771   valid's auc: 0.97102
    16. hour based alpha, beta
        grid test4:DONE
    [296]	train's auc: 0.98452	valid's auc: 0.971571
    17. feature engineering:
        aliyun: train's auc: 0.982362	valid's auc: 0.970196
    18. pub entire set kernel to compare with 17:
        azzureyun:
         train's auc: 0.980088   valid's auc: 0.967148
    19. based on 17, reduce less gain important ones
             ('ip_app_device_os_is_attributednextclick', 173517.64086914062),
             ('ip_day_hour_is_attributedcount', 191736.09509277344),
             ('ip_hour_is_attributedcount', 201711.4906616211),
             ('ip_in_test_hh_is_attributedcount', 254914.98706054688),
             ('device', 282786.8454284668),
             ('ip_device_is_attributedcount', 284967.88592529297),
             ('ip_app_hour_os_is_attributedcount', 398987.1303405762),
             ('hour', 411009.764251709),
             ('ip_app_osnunique', 425227.0106201172),
             ('app_channel_ipnunique', 755985.9567871094),
             ('ip_appnunique', 796338.018951416),
             ('ip_is_attributedcount', 1098913.5748901367),
             ('app_channel_hour_ipnunique', 1374072.0768432617),
             ('os', 1602122.4081115723),
             ('ip_channelnunique', 1690214.9694824219),
             ('channel', 21906519.45251465),
             ('app', 48804697.217926025)]
         grid test 4: train's auc: 0.982933	valid's auc: 0.969768
    20. based on 17, reduce less split important ones
         ('ip_device_os_app_is_attributedcount', 40),
         ('ip_appnunique', 46),
         ('ip_is_attributedcount', 47),
         ('app_channel_ipnunique', 49),
         ('ip_app_device_os_is_attributednextclick', 50),
         ('ip_device_is_attributedcount', 51),
         ('ip_app_hour_os_is_attributedcount', 55),
         ('hour', 137),
         ('os', 253),
         ('app', 297),
         ('channel', 484)]
        grid test:train's auc: 0.983361	valid's auc: 0.971509
        rerun to test reproducibility:
            azzureyun: train's auc: 0.983537	valid's auc: 0.971942
    21. use my orinigal fts to compare
        azzure: train's auc: 0.982564	valid's auc: 0.970444
    22. use my original nodays fts to compare
        grid test3: train's auc: 0.982597	valid's auc: 0.971147
    23. more data (from 0)with my original no days:
        aliyun: train's auc: 0.981528	valid's auc: 0.971143
    24. more data (from 0) with 20's  fts
        azzure: train's auc: 0.981043	valid's auc: 0.969882
    25. more data with pub kernel's fts:
        grid test: train's auc: 0.980178	valid's auc: 0.969537
    26. more data joint training with my original no days:
        azzure: 	train's auc: 0.979983	valid's auc: 0.970402
    27. more data joint training with 20's fts no days:
        grid test4: train's auc: 0.98176	valid's auc: 0.971532
        rerun to check reproducibility:
            aliyun: train's auc: 0.982099	valid's auc: 0.971486
    28. more data joint training with pub kern's fts:
        grid test 2: train's auc: 0.980688	valid's auc: 0.96973
    29. more data with my original with days:
        grid test: train's auc: 0.980892	valid's auc: 0.970022
    30. more data with pub fts, pub lgbm params:
        gird test3: train's auc: 0.977917	valid's auc: 0.968429
    31. more data joint training with pub kern's fts and pub kern's lgbm params:
        train's auc: 0.979741	valid's auc: 0.969463


    33. based on 20:
        increase val data to 1/2 sample to improve reproducibility:
        azzureyun: [245] train's auc: 0.982339	valid's auc: 0.972163
        grid test: [299]	train's auc: 0.982949	valid's auc: 0.972732
        grid test3: [305]	train's auc: 0.983236	valid's auc: 0.972263
        aliyun: running
    34. based on 27 - 20'fts:
        similar to 33 to test reproducibility:
        aliyun:
        gyun: running
        azzure: running
        grid test: running
        grid test3: running
    35. based on 26 - original no days:
        similar to 33 to test reproducibility:
        aliyun:
        gyun:
        azzure:
        grid test:
        grid test3:

127. ensembles:
    1. ensemble of 123.0 + 124.3
    aliyun: LB:0.9795
    2. ensemble of 124.9 + 123.0 + 124.3
    aliyun: LB:0.9797

128. LGBM params Search based on 126 setting
    only check 50 iters, and reduce search space:
    gyun: DONE
    Model #114
Best ROC-AUC: 0.9731
Best params: {'colsample_bytree': 0.8793460386326015, 'learning_rate': 0.19814501809928017, 'max_depth': 9,
    'min_child_samples': 188, 'min_child_weight': 4, 'num_leaves': 11, 'reg_alpha': 0.02387225386312356,
    'reg_lambda': 1.2196200544739068e-09, 'scale_pos_weight': 231.48637373544372,
    'subsample': 0.7079619705989065}
    LB: 0.9783

129. adversial validation model:
    1. run: pub ker
    adversial val: [('device', 301), ('os', 1064), ('app', 1103), ('channel', 1563)]
    2. run: pub ker
    second run: [('device', 41),
     ('os', 266),
     ('hour', 324),
     ('app', 423),
     ('channel', 507),
     ('ip', 964)]
130. ensemble:
    LGBM models:
    1. best single lgbm model:                                      124_3:      0.9793      X
    2. lgbm + pub ftrl fts:                                         124_9:      0.9785      X
    3. best nunique fts:                                            124_11:     0.9787      X
    4. 07/08/09 each train a model then merge(5:3:2):               124_20:     0.9795      X
    5. weight pos/neg according to ad val model, normalized:        124_22A:    0.9790      X -ignored
    6. best nunique and count from coms search:                     124_28:     0.9779      X -ignored
    7. hourly alpha 1day before smooth cvr ft, best coms search:    124_37:     0.9790      X
    8. new lgbm params:                                             124_31:     0.9786      X -ignored
    ensembled file:  ensemble_submission.csv.03-05-2018_04-04-06_lgbm_SCALE_500
    0.9795(with 124_30 instead of 124_37)
    ignored: 5,6,8, replaced 124_30 with 124_37:
    ensemble_submission.csv.03-05-2018_06-09-15: LB:0.9797

    FFM models:
    1. log discretization, seperately:                              117_8:      0.9780      X
    2. 7 raw LGBM fts:                                              117_11:     0.9775      X
    ensembled file:

    Streaming models:
    1. pub kernel:                                                  123_0:      0.9769      X

    -----------------
    total ensemble:
    1. 0.5:0.3:0.2:
    azzure: LB:0.9584
    2. ensemble of lgbm_2 + streaming: LB: 0.9796
    3. 2+117_8
    4. lgbm2*0.6 + 117_8*0.4: LB: 0.9780
    5. lgbm2*0.8 + 123_0*0.2: LB: 0.9799
    6. lgbm2*0.7 + 123_0*0.3: LB: 0.9799
    7. lgbm2*0.9 + 123_0*0.1: LB: 0.9799
    8. 124_3*0.3 + 124_20*0.3 + 124_37*0.2 + 123_0 * 0.2: LB: 0.9800
    9. 130_8 * 0.95 + 117_8 * 0.05: LB: 0.9800
    10. 9 -> 0.85 /0.15 : LB:0.9798
    11. try normailzation before ensemble.
    12. 130_8 * 0.9 + 117_8 * 0.1: 0.9765
    13.  124_3*0.2 + 124_20*0.3 + 124_37*0.2 + 123_0 * 0.3:  0.9800:
    14 123_28*0.1 +  124_3*0.2 + 124_20*0.3 + 124_37*0.2 + 123_0 * 0.2: submitting:
    15. 14 + 124_9 *0.05, 124_20->0.25 : TODO
    16. 13 -> 123_0->0.4, 124_3->0.1: 0.9800
    17. 123_0->0.5, 124_20->0.25, 124_37->0.15 0.9800
    18. 123_0->0.65, 124_20 -> 0.2, 124_
    19. 7 + normailized
    20. 124_3*0.375 + 124_20*0.375 + 124_37*0.25 based on 13 = 0.9798
    21. ensemble 8 lgbm models: 0.9797

131: use 7_4am to 7_3pm (34308844 lines) to generate val scores and train stacking LR model to stack:
    1. stacking val score for 117_8:
        aliyun: bugg'ed sampled.
        rerun-python part first: DONE
        gen new new_test.sp again(typo, should be renamed to new_val.sp later):DONE
        train and score stacking val: DONE
    2. skip 7's data in 123_0, and score 7's data for stacking:
        pub kernel:OOM
        gyun: Failed bug in the rcount==130000000 trick:
        A. try remove this trick to train all the data:
            gyun: DONE
            need to submit once trained to check the score first: LB:0.9751
        B.
    3. based on 124_3:  grid test 4: OOM
                        azzureyun: DONE
    4. based on 124_9:  grid test 2: DONE
    5. based on 124_11: grid test 3: DONE
    6. based on 124_17: grid test: OOM
        azzureyun: DONE
    7. based on 124_37: azzureyun: DONE
    8. based on 117_11: aliyun: DONE

    Stacking:
        azzureyun: 131_1 gen'ed, LB:0.9794
    9. new stacking of 123_0/124_40: LB:0.9792
    10. new stacking 8:2 lgb 117_8: 0.9795

132 config based ensemble:
    1. LB: 0.9800
    2. added new 124_42, 15min multiscale: LB: 0.9800
    3. increase small models: LB: 0.9799
    4. 132_2 * 0.65 + pub: 140_1 * 0.35:
    5. 132_2 * 0.6 + pub: 140_1 * 0.4:

133 9 as validation, 7/8 to train, jointly training, fts search:
    1. baseline: no day features:
        aliyun: OOM
        reduce mem, only use 8 4-15 to train: rerun:
        [377]	train's auc: 0.98743	valid's auc: 0.980783
    2. fts search, reduced split:
        azzureyun: OOM
    3. 2 + smooth cvr
    4. original no day + scvr to validate
        gyun: running
    5. based on 1, use day 7 and 8 hour:4-15:
        aliyun: running


to do test:
1. kaggle public kernel, most least frequent hours feature DONE
2. break down auc by dimensions and filter data and train separately and ensemble DONE
3. bag features
4. gbdt + ffm
5. gbdt feature + ffm
6. normalization
7. count smoothing: xxxx
8. unbalance labels
9. drop ip count feature, who's very overfitting according to ffm work DONE
10. looking at some channel only, performs badly, consider remove channel+ip count feature that over-fits
11. cvr confidence smooth features
12. feature engineering
13. DNN grid param tunning
14. grid test:
    train/val ranges;
    id features, st features,
    lgbm params
15. for group by dimensions, next_click times forms a sequience, the mean, var skew of this sequence
16. TODO: try only train from night hours.
17. TODO: clustering categorical features, and use clusters instead of each single value(given 2hours might work)
18. TODO: in non-tree models, remove too rare categorical items
19. TODO: cat_smooth in LGBM
20. TODO: similar to weighting in pick hours, give more recent labels bigger weight
______________________
21. TODO: most/least frequent other dimensions in test (based on most important fts from adversial validation)
adversial val: [('device', 301), ('os', 1064), ('app', 1103), ('channel', 1563)]
second run: [('device', 41),
 ('os', 266),
 ('hour', 324),
 ('app', 423),
 ('channel', 507),
 ('ip', 964)]
22. TODO: sklearn ft selection
23. TODO: in coms fts search, should set L1 much bigger
24. TODO: train a model every day, then merge.
