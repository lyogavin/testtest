{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# coding: utf-8\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import time\n",
    "\n",
    "\n",
    "def get_dated_filename(filename):\n",
    "    return '{}.{}_{}'.format(filename, time.strftime(\"%d-%m-%Y\"), time.strftime(\"%X\"))\n",
    "\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "print(os.listdir(\"../input\"))\n",
    "\n",
    "\n",
    "\n",
    "# Any results you write to the current directory are saved as output.\n",
    "\n",
    "'''\n",
    "Another CTR comp and so i suspect libffm will play its part, after all it is an atomic bomb for this kind of stuff.\n",
    "A sci-kit learn inspired script to convert pandas dataframes into libFFM style data.\n",
    "\n",
    "The script is fairly hacky (hey thats Kaggle) and takes a little while to run a huge dataset.\n",
    "The key to using this class is setting up the features dtypes correctly for output (ammend transform to suit your needs)\n",
    "\n",
    "Example below\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import lightgbm as lgb\n",
    "import sys\n",
    "import gc\n",
    "\n",
    "use_sample = False\n",
    "\n",
    "gen_test_input = True\n",
    "\n",
    "path = '../input/' \n",
    "path_train = path + 'train.csv'\n",
    "path_train_sample = path + 'train_sample.csv'\n",
    "path_test = path + 'test.csv'\n",
    "path_test_sample = path + 'test_sample.csv'\n",
    "\n",
    "train_cols = ['ip', 'app', 'device', 'os', 'channel', 'click_time', 'is_attributed']\n",
    "test_cols = ['ip', 'app', 'device', 'os', 'channel', 'click_time']\n",
    "\n",
    "categorical = ['app', 'device', 'os', 'channel', 'hour']\n",
    "\n",
    "dtypes = {\n",
    "        'ip'            : 'uint32',\n",
    "        'app'           : 'uint16',\n",
    "        'device'        : 'uint16',\n",
    "        'os'            : 'uint16',\n",
    "        'channel'       : 'uint16',\n",
    "        'is_attributed' : 'uint8',\n",
    "        'click_id'      : 'uint32'\n",
    "        }\n",
    "        \n",
    "skip = range(1, 140000000)\n",
    "print(\"Loading Data\")\n",
    "#skiprows=skip,\n",
    "\n",
    "import pickle\n",
    "\n",
    "\n",
    "# In[2]:\n",
    "\n",
    "def prepare_data(data, training_day, profile_days, sample_count=1,\n",
    "                 with_hist_profile=True, only_for_ip_with_hist = True):\n",
    "    if sample_count != 1:\n",
    "        #sample 1/4 of the data:\n",
    "        data = data.set_index('ip').loc[lambda x: (x.index + 401) % sample_count == 0].reset_index()\n",
    "        len_train = len(data)\n",
    "        print('len after sample:', len_train)\n",
    "\n",
    "    train_ip_contains_training_day = None\n",
    "    train_ip_contains_training_day_attributed = None\n",
    "\n",
    "    if with_hist_profile:\n",
    "        train_ip_contains_training_day = data.groupby('ip').filter(lambda x: x['day'].max() == training_day)\n",
    "\n",
    "        print('train_ip_contains_training_day', train_ip_contains_training_day)\n",
    "        print('train_ip_contains_training_day unique ips:', len(train_ip_contains_training_day['ip'].unique()))\n",
    "\n",
    "        if only_for_ip_with_hist:\n",
    "            data = train_ip_contains_training_day.groupby('ip').filter(lambda x: x['day'].min() < training_day)\n",
    "\n",
    "        train_ip_contains_training_day = train_ip_contains_training_day  \\\n",
    "            .query('day < {0} & day > {1}'.format(training_day, training_day - 1 - profile_days) )\n",
    "        print('train_ip_contains_training_day unique ips:', len(train_ip_contains_training_day['ip'].unique()))\n",
    "\n",
    "        print('split attributed data:')\n",
    "        train_ip_contains_training_day_attributed = train_ip_contains_training_day.query('is_attributed == 1')\n",
    "        print('len:',len(train_ip_contains_training_day_attributed))\n",
    "\n",
    "    #only use data on 9 to train, but data before 9 as features\n",
    "    train = data.query('day == {}'.format(training_day))\n",
    "    print('training data len:', len(train))\n",
    "    \n",
    "    return train, \\\n",
    "           train_ip_contains_training_day, train_ip_contains_training_day_attributed\n",
    "\n",
    "\n",
    "def add_statistic_feature(group_by_cols, training, training_hist, training_hist_attribution,\n",
    "                          with_hist, counting_col='channel', cast_type=False, qcut_count=0, discretization=0):\n",
    "    features_added = []\n",
    "    feature_name_added = '_'.join(group_by_cols) + 'count'\n",
    "    print('count ip with group by:', group_by_cols)\n",
    "    n_chans = training[group_by_cols + [counting_col]].groupby(by=group_by_cols)[[counting_col]] \\\n",
    "        .count().reset_index().rename(columns={counting_col: feature_name_added})\n",
    "    training = training.merge(n_chans, on=group_by_cols, how='left')\n",
    "    del n_chans\n",
    "    gc.collect()\n",
    "    training[feature_name_added] = training[feature_name_added].astype('uint16')\n",
    "    if qcut_count != 0:\n",
    "        print('before qcut', feature_name_added, training[feature_name_added].describe())\n",
    "        quantile_cut = training[feature_name_added].quantile(qcut_count)\n",
    "        training[feature_name_added] = training[feature_name_added].apply(\n",
    "            lambda x: x if x < quantile_cut else 65535).astype('uint16')\n",
    "        print('after qcut', feature_name_added, training[feature_name_added].describe())\n",
    "    if discretization != 0:\n",
    "        print('before qcut', feature_name_added, training[feature_name_added].describe())\n",
    "        training[feature_name_added] = pd.qcut(training[feature_name_added], discretization, labels=False,\n",
    "                                               duplicates='drop').fillna(0).astype('uint16')\n",
    "        print('after qcut', feature_name_added, training[feature_name_added].describe())\n",
    "\n",
    "    features_added.append(feature_name_added)\n",
    "\n",
    "    if with_hist:\n",
    "        print('count ip with group by in hist data:', group_by_cols)\n",
    "        feature_name_added = '_'.join(group_by_cols) + \"count_in_hist\"\n",
    "        n_chans = training_hist[group_by_cols + [counting_col]].groupby(by=group_by_cols)[[counting_col]] \\\n",
    "            .count().reset_index().rename(columns={counting_col: feature_name_added})\n",
    "        training = training.merge(n_chans, on=group_by_cols, how='left')\n",
    "        del n_chans\n",
    "        gc.collect()\n",
    "        print('count ip attribution with group by in hist data:', group_by_cols)\n",
    "        feature_name_added1 = '_'.join(group_by_cols) + \"count_attribution_in_hist\"\n",
    "        n_chans = training_hist_attribution[group_by_cols + [counting_col]] \\\n",
    "            .groupby(by=group_by_cols)[[counting_col]] \\\n",
    "            .count().reset_index().rename(columns={counting_col: feature_name_added1})\n",
    "        training = training.merge(n_chans, on=group_by_cols, how='left')\n",
    "        del n_chans\n",
    "        gc.collect()\n",
    "\n",
    "        feature_name_added2 = '_'.join(group_by_cols) + \"count_attribution_rate_in_hist\"\n",
    "        training[feature_name_added2] = \\\n",
    "            training[feature_name_added1] / training[feature_name_added] * 1000.0\n",
    "\n",
    "        if qcut_count != 0:\n",
    "            print('before qcut', feature_name_added, training[feature_name_added].describe())\n",
    "            quantile_cut = training[feature_name_added].quantile(qcut_count)\n",
    "            training[feature_name_added] = training[feature_name_added].apply(lambda x: x if x < quantile_cut else -1)\n",
    "            print('after qcut', feature_name_added, training[feature_name_added].describe())\n",
    "\n",
    "        if cast_type:\n",
    "            training[feature_name_added] = training[feature_name_added].fillna(0).astype('uint16')\n",
    "        if discretization != 0:\n",
    "            print('before qcut', feature_name_added, training[feature_name_added].describe())\n",
    "            training[feature_name_added] = pd.qcut(training[feature_name_added], discretization, labels=False,\n",
    "                                                   duplicates='drop').fillna(0).astype('uint16')\n",
    "            print('after qcut', feature_name_added, training[feature_name_added].describe())\n",
    "\n",
    "\n",
    "\n",
    "        if qcut_count != 0:\n",
    "            print('before qcut', feature_name_added1, training[feature_name_added1].describe())\n",
    "            quantile_cut = training[feature_name_added1].quantile(qcut_count)\n",
    "            training[feature_name_added1] = training[feature_name_added1].apply(lambda x: x if x < quantile_cut else -1)\n",
    "            print('after qcut', feature_name_added1, training[feature_name_added1].describe())\n",
    "\n",
    "        if cast_type:\n",
    "            training[feature_name_added1] = training[feature_name_added1].fillna(0).astype('uint16')\n",
    "            #training = training.astype({feature_name_added1:'uint16'})\n",
    "            print(training[feature_name_added1])\n",
    "        if discretization != 0:\n",
    "            print('before qcut', feature_name_added1, training[feature_name_added1].describe())\n",
    "            training[feature_name_added1] = pd.qcut(training[feature_name_added1], discretization, labels=False,\n",
    "                                                    duplicates='drop').fillna(0).astype('uint16')\n",
    "            print('after qcut', feature_name_added1, training[feature_name_added1].describe())\n",
    "        # training[feature_name_added1] = training[feature_name_added1].astype('uint16')\n",
    "\n",
    "\n",
    "        if cast_type:\n",
    "            training[feature_name_added2] = training[feature_name_added2].fillna(0).astype('uint16')\n",
    "\n",
    "        features_added.append(feature_name_added)\n",
    "        features_added.append(feature_name_added1)\n",
    "        features_added.append(feature_name_added2)\n",
    "\n",
    "    print('added features:', features_added)\n",
    "\n",
    "    return training, features_added\n",
    "\n",
    "def generate_counting_history_features(data, history, history_attribution,\n",
    "                                       with_hist_profile = True, remove_hist_profile_count=2):\n",
    "        \n",
    "    new_features = []\n",
    "\n",
    "    # Count by IP,DAY,HOUR\n",
    "    print('a given IP address within each hour...')\n",
    "    data, features_added = add_statistic_feature(['ip','day','hour'], data, history, history_attribution, False)\n",
    "    new_features = new_features + features_added\n",
    "    gc.collect()\n",
    "\n",
    "    # Count by IP and APP\n",
    "    data, features_added = add_statistic_feature(['ip','app'], data, history, history_attribution, with_hist_profile)\n",
    "    new_features = new_features + features_added\n",
    "    \n",
    "    # Count by IP and channel\n",
    "    data, features_added = add_statistic_feature(['ip','channel'], data, history, history_attribution, with_hist_profile, counting_col='os')\n",
    "    new_features = new_features + features_added\n",
    "    \n",
    "    # Count by IP and channel app\n",
    "    data, features_added = add_statistic_feature(['ip','channel', 'app'], data, history, history_attribution, with_hist_profile, counting_col='os')\n",
    "    new_features = new_features + features_added\n",
    "    \n",
    "    data, features_added  = add_statistic_feature(['ip','app','os'], data, history, history_attribution, with_hist_profile)\n",
    "    new_features = new_features + features_added\n",
    "\n",
    "    #######\n",
    "    # Count by IP\n",
    "    data, features_added  = add_statistic_feature(['ip'], data, history, history_attribution, with_hist_profile)\n",
    "    new_features = new_features + features_added\n",
    "\n",
    "    # Count by IP HOUR CHANNEL                                               \n",
    "    data, features_added  = add_statistic_feature(['ip','hour','channel'],\n",
    "                                                  data, history, history_attribution, with_hist_profile, counting_col='os')\n",
    "    new_features = new_features + features_added\n",
    "\n",
    "    # Count by IP HOUR Device\n",
    "    data, features_added  = add_statistic_feature(['ip','hour','os'],\n",
    "                                                  data, history, history_attribution, with_hist_profile)\n",
    "    new_features = new_features + features_added\n",
    "\n",
    "    data, features_added  = add_statistic_feature(['ip','hour','app'],\n",
    "                                                  data, history, history_attribution, with_hist_profile)\n",
    "    new_features = new_features + features_added\n",
    "\n",
    "    data, features_added  = add_statistic_feature(['ip','hour','device'],\n",
    "                                                  data, history, history_attribution, with_hist_profile)\n",
    "    new_features = new_features + features_added\n",
    "\n",
    "\n",
    "    if remove_hist_profile_count != 0:\n",
    "        data = data.query('ipcount_in_hist > {}'.format(remove_hist_profile_count))\n",
    "\n",
    "    return data, new_features\n",
    "\n",
    "#test['hour'] = test[\"click_time\"].dt.hour.astype('uint8')\n",
    "#test['day'] = test[\"click_time\"].dt.day.astype('uint8')\n",
    "\n",
    "\n",
    "\n",
    "def gen_train_df(with_hist_profile = True):\n",
    "    train = pd.read_csv(path_train_sample if use_sample else path_train, dtype=dtypes,\n",
    "            header=0,usecols=train_cols,parse_dates=[\"click_time\"])#.sample(1000)\n",
    "\n",
    "\n",
    "    len_train = len(train)\n",
    "    print('The initial size of the train set is', len_train)\n",
    "    print('Binding the training and test set together...')\n",
    "\n",
    "\n",
    "    print(\"Creating new time features in train: 'hour' and 'day'...\")\n",
    "    train['hour'] = train[\"click_time\"].dt.hour.astype('uint8')\n",
    "    train['day'] = train[\"click_time\"].dt.day.astype('uint8')\n",
    "\n",
    "    train_data, train_ip_contains_training_day, train_ip_contains_training_day_attributed =  \\\n",
    "        prepare_data(train, 9, 2, 4, with_hist_profile)\n",
    "\n",
    "    train_data, new_features = generate_counting_history_features(train_data, train_ip_contains_training_day,\n",
    "                                                                  train_ip_contains_training_day_attributed,\n",
    "                                                                  with_hist_profile)\n",
    "\n",
    "    print('train data:', train)\n",
    "    print('new features:', new_features)\n",
    "    print('train data ip count in hist:', train_data['ipcount_in_hist'].describe())\n",
    "    print('train data min ', train_ip_contains_training_day.groupby('ip')['day'].min())\n",
    "\n",
    "    #gen val data:\n",
    "    #val = train.set_index('ip').loc[lambda x: (x.index) % 17 == 0].reset_index()\n",
    "    #print(val)\n",
    "    #print('The size of the validation set is ', len(val))\n",
    "\n",
    "    del train_ip_contains_training_day\n",
    "    del train_ip_contains_training_day_attributed\n",
    "    gc.collect()\n",
    "\n",
    "    val, train_ip_contains_training_day, train_ip_contains_training_day_attributed =  \\\n",
    "        prepare_data(train, 8, 2, 10, with_hist_profile)\n",
    "\n",
    "    val, new_features = generate_counting_history_features(val, train_ip_contains_training_day,\n",
    "                                                           train_ip_contains_training_day_attributed,\n",
    "                                                           with_hist_profile)\n",
    "\n",
    "    train = train_data\n",
    "\n",
    "    del train_ip_contains_training_day\n",
    "    del train_ip_contains_training_day_attributed\n",
    "    del train_data\n",
    "    gc.collect()\n",
    "    #train = train.set_index('ip').loc[lambda x: (x.index) % 17 != 0].reset_index()\n",
    "    #print('The size of the train set is ', len(train))\n",
    "\n",
    "    target = 'is_attributed'\n",
    "    train[target] = train[target].astype('uint8')\n",
    "    train.info()\n",
    "\n",
    "    if use_sample:\n",
    "        train.to_csv(get_dated_filename('training_sample.csv'), index=False)\n",
    "        val.to_csv(get_dated_filename('val_sample.csv'), index=False)\n",
    "    else:\n",
    "        train.to_csv(get_dated_filename('training.csv'), index=False)\n",
    "        val.to_csv(get_dated_filename('val.csv'), index=False)\n",
    "\n",
    "    print('save dtypes')\n",
    "\n",
    "    y = {k: str(v) for k, v in train.dtypes.to_dict().items()}\n",
    "    print(y)\n",
    "    del y['click_time']\n",
    "    #del y['Unnamed: 0']\n",
    "    pickle.dump(y,open('output_dtypes.pickle','wb'))\n",
    "\n",
    "    #sys.exit(0)\n",
    "    return train, val, new_features\n",
    "\n",
    "\n",
    "train_lgbm = False\n",
    "\n",
    "def train_lgbm(train, val, new_features):\n",
    "#if train_lgbm:\n",
    "\n",
    "    # In[7]:\n",
    "    target = 'is_attributed'\n",
    "\n",
    "    predictors0 = ['device', 'app', 'os', 'channel', 'hour', # Starter Vars, Then new features below\n",
    "                  'ip_day_hourcount','ipcount','ip_appcount', 'ip_app_oscount',\n",
    "                  \"ip_hour_channelcount\", \"ip_hour_oscount\", \"ip_hour_appcount\",\"ip_hour_devicecount\"]\n",
    "\n",
    "    predictors1 = categorical + new_features\n",
    "    #for ii in new_features:\n",
    "    #    predictors1 = predictors1 + ii\n",
    "    #print(predictors1)\n",
    "    gc.collect()\n",
    "\n",
    "    #train.fillna(value={x:-1 for x in new_features})\n",
    "\n",
    "    print(\"Preparing the datasets for training...\")\n",
    "\n",
    "    params = {\n",
    "        'boosting_type': 'gbdt',\n",
    "        'objective': 'binary',\n",
    "        'metric': 'auc',\n",
    "        'learning_rate': 0.1,\n",
    "        'num_leaves': 7,\n",
    "        'max_depth': 4,\n",
    "        'min_child_samples': 100,\n",
    "        'max_bin': 150,\n",
    "        'subsample': 0.7,\n",
    "        'subsample_freq': 1,\n",
    "        'colsample_bytree': 0.7,\n",
    "        'min_child_weight': 0,\n",
    "        'subsample_for_bin': 200000,\n",
    "        'min_split_gain': 0,\n",
    "        'reg_alpha': 0,\n",
    "        'reg_lambda': 0,\n",
    "        'nthread': 5,\n",
    "        'verbose': 9,\n",
    "        #'is_unbalance': True,\n",
    "        'scale_pos_weight':99\n",
    "        }\n",
    "\n",
    "    predictors_to_train = [predictors1]\n",
    "\n",
    "    for predictors in predictors_to_train:\n",
    "        print('training with :', predictors)\n",
    "        #print('training data: ', train[predictors].values)\n",
    "        #print('validation data: ', val[predictors].values)\n",
    "        dtrain = lgb.Dataset(train[predictors].values, label=train[target].values,\n",
    "                              feature_name=predictors,\n",
    "                              categorical_feature=categorical\n",
    "                              )\n",
    "        dvalid = lgb.Dataset(val[predictors].values, label=val[target].values,\n",
    "                              feature_name=predictors,\n",
    "                              categorical_feature=categorical\n",
    "                              )\n",
    "\n",
    "        evals_results = {}\n",
    "        print(\"Training the model...\")\n",
    "\n",
    "        lgb_model = lgb.train(params,\n",
    "                         dtrain,\n",
    "                         valid_sets=[dtrain, dvalid],\n",
    "                         valid_names=['train','valid'],\n",
    "                         evals_result=evals_results,\n",
    "                         num_boost_round=1000,\n",
    "                         early_stopping_rounds=30,\n",
    "                         verbose_eval=50,\n",
    "                         feval=None)\n",
    "\n",
    "        #del train\n",
    "        #del val\n",
    "        #gc.collect()\n",
    "\n",
    "        # Nick's Feature Importance Plot\n",
    "        import matplotlib.pyplot as plt\n",
    "        f, ax = plt.subplots(figsize=[7,10])\n",
    "        lgb.plot_importance(lgb_model, ax=ax, max_num_features=len(predictors))\n",
    "        plt.title(\"Light GBM Feature Importance\")\n",
    "        plt.savefig('feature_import.png')\n",
    "\n",
    "        # Feature names:\n",
    "        print('Feature names:', lgb_model.feature_name())\n",
    "        # Feature importances:\n",
    "        print('Feature importances:', list(lgb_model.feature_importance()))\n",
    "\n",
    "        feature_imp = pd.DataFrame(lgb_model.feature_name(),list(lgb_model.feature_importance()))\n",
    "\n",
    "        lgb_model.save_model(get_dated_filename('model.txt'))\n",
    "\n",
    "        print('gen val prediction')\n",
    "        val_prediction = lgb_model.predict(val[predictors1], num_iteration=lgb_model.best_iteration)\n",
    "\n",
    "        print(\"Writing the val_prediction into a csv file...\")\n",
    "\n",
    "        pd.Series(val_prediction).to_csv(get_dated_filename(\"val_prediction.csv\"), index=False)\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "for_test = True\n",
    "\n",
    "def gen_test_df(with_hist_profile = True):\n",
    "    #del train\n",
    "    #del test\n",
    "    #gc.collect()\n",
    "\n",
    "    #prepare test data:\n",
    "    if with_hist_profile:\n",
    "        train = pd.read_csv(path_train, dtype=dtypes,\n",
    "                header=0,usecols=train_cols,parse_dates=[\"click_time\"])#.sample(1000)\n",
    "    test = pd.read_csv(path_test if not use_sample else path_test_sample, dtype=dtypes, header=0,\n",
    "            usecols=test_cols,parse_dates=[\"click_time\"])#.sample(1000)\n",
    "    if with_hist_profile:\n",
    "        train=train.append(test)\n",
    "    else:\n",
    "        train = test\n",
    "    del test\n",
    "    gc.collect()\n",
    "    print(\"Creating new time features in train: 'hour' and 'day'...\")\n",
    "    train['hour'] = train[\"click_time\"].dt.hour.astype('uint8')\n",
    "    train['day'] = train[\"click_time\"].dt.day.astype('uint8')\n",
    "    \n",
    "    train, train_ip_contains_training_day, train_ip_contains_training_day_attributed = \\\n",
    "        prepare_data(train, 10, 3, 1, with_hist_profile)\n",
    "\n",
    "    train, new_features = generate_counting_history_features(train, train_ip_contains_training_day, \n",
    "                                                             train_ip_contains_training_day_attributed,\n",
    "                                                             with_hist_profile)\n",
    "\n",
    "    train['is_attributed'] = 0\n",
    "    train.to_csv(get_dated_filename('to_submit.csv' + '.sample' if use_sample else ''), index=False)\n",
    "\n",
    "    return train, new_features\n",
    "\n",
    "# In[ ]:\n",
    "to_submit = False\n",
    "\n",
    "if to_submit:\n",
    "    print('test data:', train)\n",
    "\n",
    "    print('new features:', new_features)\n",
    "    print(\"Preparing data for submission...\")\n",
    "\n",
    "    submit = pd.read_csv(path_test, dtype='int', usecols=['click_id'])\n",
    "    print('submit test len:', len(submit))\n",
    "    print(\"Predicting the submission data...\")\n",
    "    submit['is_attributed'] = lgb_model.predict(train[predictors1], num_iteration=lgb_model.best_iteration)\n",
    "\n",
    "    print(\"Writing the submission data into a csv file...\")\n",
    "\n",
    "    submit.to_csv(get_dated_filename(\"submission_notebook.csv\"),index=False)\n",
    "\n",
    "    print(\"All done...\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def gen_ffm_data():\n",
    "    train, val, new_features = gen_train_df(False)\n",
    "    train_len = len(train)\n",
    "    val_len = len(val)\n",
    "    gc.collect()\n",
    "    test, _ = gen_test_df(False)\n",
    "    test_len= len(test)\n",
    "    gc.collect()\n",
    "\n",
    "    print('train({}) val({}) test({}) generated'.format(train_len, val_len,test_len))\n",
    "\n",
    "\n",
    "    train = train.append(val)\n",
    "    test = train.append(test)\n",
    "\n",
    "    del train\n",
    "    del val\n",
    "\n",
    "    gc.collect()\n",
    "\n",
    "    print(test)\n",
    "\n",
    "\n",
    "train_model = True\n",
    "\n",
    "if train_model:\n",
    "\n",
    "    train, val, new_features = gen_train_df(True)\n",
    "\n",
    "    train_lgbm(train, val, new_features)\n",
    "    # In[ ]:\n",
    "\n",
    "\n",
    "#gen_ffm_data()\n",
    "\n",
    "predict_from_saved_model = False\n",
    "\n",
    "if predict_from_saved_model:\n",
    "    test, new_test_features = gen_test_df(True)\n",
    "\n",
    "    print(test['ipcount_in_hist'].describe())\n",
    "\n",
    "    print(test)\n",
    "\n",
    "    lgb_saved_model = lgb.Booster(model_file='model.txt.01-04-2018_10:59:15')\n",
    "\n",
    "    submit = pd.read_csv(path_test, dtype='int', usecols=['click_id'])\n",
    "\n",
    "    predictors1 = categorical + new_test_features\n",
    "    submit['is_attributed'] = lgb_saved_model.predict(test[predictors1], num_iteration=lgb_saved_model.best_iteration)\n",
    "\n",
    "    print(\"Writing the submission data into a csv file...\")\n",
    "\n",
    "    submit.to_csv(get_dated_filename(\"submission_notebook.csv\"), index=False)\n",
    "\n",
    "    print(\"All done...\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
